{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA RTX A5000\n",
      "tensor([[0.4938, 0.5651, 0.5798],\n",
      "        [0.5265, 0.5529, 0.2071],\n",
      "        [0.1274, 0.2573, 0.5208]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "flag = torch.cuda.is_available()\n",
    "print(flag)\n",
    "\n",
    "if flag:\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(torch.rand(3,3).cuda()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 确定项目根目录（假设当前工作目录是项目的根目录）\n",
    "project_root = os.path.abspath(\"..\")\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "import pickle, re, json\n",
    "import numpy as np\n",
    "from ai import ai_battle, MinimaxAI, MCTSAI\n",
    "from game.chess_game import ChessGame\n",
    "from CelestialVault.instances import ExampleThreadManager\n",
    "from time import strftime, localtime\n",
    "\n",
    "\n",
    "class TrainDataThread(ExampleThreadManager):\n",
    "    def get_args(self, obj: object):\n",
    "        train_game = ChessGame((5, 5), 2)\n",
    "        train_game.init_cfunc()\n",
    "        train_game.init_history()\n",
    "        return (mcts_ai_0, mcts_ai_0, train_game, False)\n",
    "    \n",
    "    def process_result(self):\n",
    "        all_training_data = []\n",
    "        result_dict = self.get_result_dict()\n",
    "        for over_game in result_dict.values():\n",
    "            history_board = over_game.history_board\n",
    "            history_move = over_game.history_move\n",
    "            for step in range(over_game.max_step-1):\n",
    "                board = self.process_board(history_board[step], step)\n",
    "                # if (board, history_move[step+1]) in all_training_data: # 这样效果并不好\n",
    "                #     continue\n",
    "                all_training_data.append((board, history_move[step+1]))\n",
    "        return all_training_data\n",
    "    \n",
    "    def process_board(self, chess_board, step):\n",
    "        color = 1 if step % 2 == 0 else -1\n",
    "        color_channel = np.full((5, 5, 1), color)\n",
    "\n",
    "        processed_board = np.concatenate((chess_board, color_channel), axis=2)\n",
    "        \n",
    "        for row in processed_board:\n",
    "            for cell in row:\n",
    "                if cell[0] == float(\"inf\"):\n",
    "                    cell[0] = 5\n",
    "        return processed_board\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "def save_data(data):\n",
    "    data_size = len(data)\n",
    "    now_time = strftime(\"%m-%d-%H\", localtime())\n",
    "    pickle.dump(data, open(f\"train_data/all_training_data({now_time})({data_size}).pkl\", \"wb\"))\n",
    "\n",
    "def start_train_data(train_num):\n",
    "    train_data_threader.start(range(train_num), \"serial\")\n",
    "    train_data_threader.handle_error()\n",
    "    all_training_data = train_data_threader.process_result()\n",
    "\n",
    "    save_data(all_training_data)\n",
    "    \n",
    "    return all_training_data\n",
    "\n",
    "def get_model_info_dict(path, train_data_path, model):\n",
    "    model_info_dict = {}\n",
    "    model_info_dict[\"path\"] = path\n",
    "    model_info_dict[\"train_data_path\"] = train_data_path\n",
    "\n",
    "    model_str = str(model)\n",
    "    model_lines = re.split(r'\\n', model_str)\n",
    "    layer_dict = {}\n",
    "    for line in model_lines[1:-1]:  # 跳过开头和结尾的行\n",
    "        re_ = re.compile(r\"\\((.*?)\\): (.*)\")\n",
    "        layer_name = re_.search(line).group(1)\n",
    "        layer_args = re_.search(line).group(2)\n",
    "        \n",
    "        layer_dict[layer_name] = layer_args\n",
    "    model_info_dict[\"layers\"] = layer_dict\n",
    "\n",
    "\n",
    "    return model_info_dict\n",
    "\n",
    "def save_info_dict(info_dict, model_type):\n",
    "    with open('model_score.json', 'r') as f:\n",
    "        model_score = json.load(f)\n",
    "\n",
    "    model_score[model_type].append(info_dict)\n",
    "\n",
    "    with open('model_score.json', 'w') as f:\n",
    "        json.dump(model_score, f, indent=2)\n",
    "\n",
    "# minimax_ai = MinimaxAI(5)\n",
    "mcts_ai_0 = MCTSAI(1000, complate_mode=False)\n",
    "mcts_ai_1 = MCTSAI(50000, complate_mode=False)\n",
    "\n",
    "train_data_threader = TrainDataThread(\n",
    "            ai_battle,\n",
    "            thread_num=200,\n",
    "            tqdm_desc='trainDataProcess',\n",
    "            show_progress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "trainDataProcess: 100%|██████████| 200/200 [01:28<00:00,  2.26it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:28<00:00,  2.25it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:28<00:00,  2.25it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:28<00:00,  2.25it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:29<00:00,  2.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13631"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_training_data = []\n",
    "for _ in range(1):\n",
    "    all_training_data += train_data(1000)\n",
    "\n",
    "len(all_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.]],\n",
       " \n",
       "        [[0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.]],\n",
       " \n",
       "        [[0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.]],\n",
       " \n",
       "        [[0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.]],\n",
       " \n",
       "        [[0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.]]]),\n",
       " array([1, 2]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = r\"train_data\\train_data(06-22-17)(136090).pkl\"\n",
    "\n",
    "train_data = load_data(train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136090"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class ChessDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        board_state, move = self.data[idx]\n",
    "        board_state = torch.tensor(board_state, dtype=torch.float32)\n",
    "        move = move[0] * 5 + move[1]\n",
    "        return board_state, move\n",
    "\n",
    "dataset = ChessDataset(train_data)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100, Loss: 3.245\n",
      "Epoch 1, Batch 200, Loss: 3.245\n",
      "Epoch 1, Batch 300, Loss: 3.245\n",
      "Epoch 1, Batch 400, Loss: 3.250\n",
      "Epoch 1, Batch 500, Loss: 3.252\n",
      "Epoch 1, Batch 600, Loss: 3.239\n",
      "Epoch 1, Batch 700, Loss: 3.243\n",
      "Epoch 1, Batch 800, Loss: 3.244\n",
      "Epoch 1, Batch 900, Loss: 3.246\n",
      "Epoch 1, Batch 1000, Loss: 3.244\n",
      "Epoch 1, Batch 1100, Loss: 3.248\n",
      "Epoch 1, Batch 1200, Loss: 3.247\n",
      "Epoch 1, Batch 1300, Loss: 3.253\n",
      "Epoch 1, Batch 1400, Loss: 3.244\n",
      "Epoch 1, Batch 1500, Loss: 3.242\n",
      "Epoch 1, Batch 1600, Loss: 3.248\n",
      "Epoch 1, Batch 1700, Loss: 3.248\n",
      "Epoch 1, Batch 1800, Loss: 3.248\n",
      "Epoch 1, Batch 1900, Loss: 3.249\n",
      "Epoch 1, Batch 2000, Loss: 3.247\n",
      "Epoch 1, Batch 2100, Loss: 3.246\n",
      "Epoch 1, Batch 2200, Loss: 3.244\n",
      "Epoch 1, Batch 2300, Loss: 3.248\n",
      "Epoch 1, Batch 2400, Loss: 3.243\n",
      "Epoch 1, Batch 2500, Loss: 3.248\n",
      "Epoch 1, Batch 2600, Loss: 3.253\n",
      "Epoch 1, Batch 2700, Loss: 3.242\n",
      "Epoch 1, Batch 2800, Loss: 3.243\n",
      "Epoch 1, Batch 2900, Loss: 3.248\n",
      "Epoch 1, Batch 3000, Loss: 3.244\n",
      "Epoch 1, Batch 3100, Loss: 3.246\n",
      "Epoch 1, Batch 3200, Loss: 3.248\n",
      "Epoch 1, Batch 3300, Loss: 3.245\n",
      "Epoch 1, Batch 3400, Loss: 3.245\n",
      "Epoch 1, Batch 3500, Loss: 3.250\n",
      "Epoch 1, Batch 3600, Loss: 3.242\n",
      "Epoch 1, Batch 3700, Loss: 3.246\n",
      "Epoch 1, Batch 3800, Loss: 3.247\n",
      "Epoch 1, Batch 3900, Loss: 3.247\n",
      "Epoch 1, Batch 4000, Loss: 3.245\n",
      "Epoch 1, Batch 4100, Loss: 3.242\n",
      "Epoch 1, Batch 4200, Loss: 3.242\n",
      "Epoch 2, Batch 100, Loss: 3.248\n",
      "Epoch 2, Batch 200, Loss: 3.244\n",
      "Epoch 2, Batch 300, Loss: 3.252\n",
      "Epoch 2, Batch 400, Loss: 3.244\n",
      "Epoch 2, Batch 500, Loss: 3.251\n",
      "Epoch 2, Batch 600, Loss: 3.244\n",
      "Epoch 2, Batch 700, Loss: 3.245\n",
      "Epoch 2, Batch 800, Loss: 3.243\n",
      "Epoch 2, Batch 900, Loss: 3.246\n",
      "Epoch 2, Batch 1000, Loss: 3.252\n",
      "Epoch 2, Batch 1100, Loss: 3.243\n",
      "Epoch 2, Batch 1200, Loss: 3.247\n",
      "Epoch 2, Batch 1300, Loss: 3.252\n",
      "Epoch 2, Batch 1400, Loss: 3.241\n",
      "Epoch 2, Batch 1500, Loss: 3.248\n",
      "Epoch 2, Batch 1600, Loss: 3.240\n",
      "Epoch 2, Batch 1700, Loss: 3.249\n",
      "Epoch 2, Batch 1800, Loss: 3.245\n",
      "Epoch 2, Batch 1900, Loss: 3.237\n",
      "Epoch 2, Batch 2000, Loss: 3.221\n",
      "Epoch 2, Batch 2100, Loss: 3.217\n",
      "Epoch 2, Batch 2200, Loss: 3.217\n",
      "Epoch 2, Batch 2300, Loss: 3.215\n",
      "Epoch 2, Batch 2400, Loss: 3.214\n",
      "Epoch 2, Batch 2500, Loss: 3.213\n",
      "Epoch 2, Batch 2600, Loss: 3.215\n",
      "Epoch 2, Batch 2700, Loss: 3.213\n",
      "Epoch 2, Batch 2800, Loss: 3.211\n",
      "Epoch 2, Batch 2900, Loss: 3.213\n",
      "Epoch 2, Batch 3000, Loss: 3.210\n",
      "Epoch 2, Batch 3100, Loss: 3.213\n",
      "Epoch 2, Batch 3200, Loss: 3.209\n",
      "Epoch 2, Batch 3300, Loss: 3.207\n",
      "Epoch 2, Batch 3400, Loss: 3.200\n",
      "Epoch 2, Batch 3500, Loss: 3.206\n",
      "Epoch 2, Batch 3600, Loss: 3.198\n",
      "Epoch 2, Batch 3700, Loss: 3.195\n",
      "Epoch 2, Batch 3800, Loss: 3.187\n",
      "Epoch 2, Batch 3900, Loss: 3.187\n",
      "Epoch 2, Batch 4000, Loss: 3.187\n",
      "Epoch 2, Batch 4100, Loss: 3.184\n",
      "Epoch 2, Batch 4200, Loss: 3.178\n",
      "Epoch 3, Batch 100, Loss: 3.182\n",
      "Epoch 3, Batch 200, Loss: 3.175\n",
      "Epoch 3, Batch 300, Loss: 3.176\n",
      "Epoch 3, Batch 400, Loss: 3.166\n",
      "Epoch 3, Batch 500, Loss: 3.166\n",
      "Epoch 3, Batch 600, Loss: 3.163\n",
      "Epoch 3, Batch 700, Loss: 3.168\n",
      "Epoch 3, Batch 800, Loss: 3.166\n",
      "Epoch 3, Batch 900, Loss: 3.165\n",
      "Epoch 3, Batch 1000, Loss: 3.160\n",
      "Epoch 3, Batch 1100, Loss: 3.158\n",
      "Epoch 3, Batch 1200, Loss: 3.150\n",
      "Epoch 3, Batch 1300, Loss: 3.156\n",
      "Epoch 3, Batch 1400, Loss: 3.149\n",
      "Epoch 3, Batch 1500, Loss: 3.150\n",
      "Epoch 3, Batch 1600, Loss: 3.145\n",
      "Epoch 3, Batch 1700, Loss: 3.140\n",
      "Epoch 3, Batch 1800, Loss: 3.151\n",
      "Epoch 3, Batch 1900, Loss: 3.142\n",
      "Epoch 3, Batch 2000, Loss: 3.131\n",
      "Epoch 3, Batch 2100, Loss: 3.137\n",
      "Epoch 3, Batch 2200, Loss: 3.134\n",
      "Epoch 3, Batch 2300, Loss: 3.135\n",
      "Epoch 3, Batch 2400, Loss: 3.141\n",
      "Epoch 3, Batch 2500, Loss: 3.123\n",
      "Epoch 3, Batch 2600, Loss: 3.132\n",
      "Epoch 3, Batch 2700, Loss: 3.121\n",
      "Epoch 3, Batch 2800, Loss: 3.134\n",
      "Epoch 3, Batch 2900, Loss: 3.135\n",
      "Epoch 3, Batch 3000, Loss: 3.127\n",
      "Epoch 3, Batch 3100, Loss: 3.115\n",
      "Epoch 3, Batch 3200, Loss: 3.120\n",
      "Epoch 3, Batch 3300, Loss: 3.109\n",
      "Epoch 3, Batch 3400, Loss: 3.126\n",
      "Epoch 3, Batch 3500, Loss: 3.121\n",
      "Epoch 3, Batch 3600, Loss: 3.120\n",
      "Epoch 3, Batch 3700, Loss: 3.112\n",
      "Epoch 3, Batch 3800, Loss: 3.123\n",
      "Epoch 3, Batch 3900, Loss: 3.111\n",
      "Epoch 3, Batch 4000, Loss: 3.109\n",
      "Epoch 3, Batch 4100, Loss: 3.115\n",
      "Epoch 3, Batch 4200, Loss: 3.099\n",
      "Epoch 4, Batch 100, Loss: 3.092\n",
      "Epoch 4, Batch 200, Loss: 3.095\n",
      "Epoch 4, Batch 300, Loss: 3.088\n",
      "Epoch 4, Batch 400, Loss: 3.084\n",
      "Epoch 4, Batch 500, Loss: 3.084\n",
      "Epoch 4, Batch 600, Loss: 3.083\n",
      "Epoch 4, Batch 700, Loss: 3.074\n",
      "Epoch 4, Batch 800, Loss: 3.082\n",
      "Epoch 4, Batch 900, Loss: 3.066\n",
      "Epoch 4, Batch 1000, Loss: 3.084\n",
      "Epoch 4, Batch 1100, Loss: 3.060\n",
      "Epoch 4, Batch 1200, Loss: 3.079\n",
      "Epoch 4, Batch 1300, Loss: 3.068\n",
      "Epoch 4, Batch 1400, Loss: 3.074\n",
      "Epoch 4, Batch 1500, Loss: 3.067\n",
      "Epoch 4, Batch 1600, Loss: 3.056\n",
      "Epoch 4, Batch 1700, Loss: 3.069\n",
      "Epoch 4, Batch 1800, Loss: 3.051\n",
      "Epoch 4, Batch 1900, Loss: 3.050\n",
      "Epoch 4, Batch 2000, Loss: 3.066\n",
      "Epoch 4, Batch 2100, Loss: 3.073\n",
      "Epoch 4, Batch 2200, Loss: 3.050\n",
      "Epoch 4, Batch 2300, Loss: 3.052\n",
      "Epoch 4, Batch 2400, Loss: 3.041\n",
      "Epoch 4, Batch 2500, Loss: 3.064\n",
      "Epoch 4, Batch 2600, Loss: 3.055\n",
      "Epoch 4, Batch 2700, Loss: 3.063\n",
      "Epoch 4, Batch 2800, Loss: 3.057\n",
      "Epoch 4, Batch 2900, Loss: 3.062\n",
      "Epoch 4, Batch 3000, Loss: 3.065\n",
      "Epoch 4, Batch 3100, Loss: 3.042\n",
      "Epoch 4, Batch 3200, Loss: 3.046\n",
      "Epoch 4, Batch 3300, Loss: 3.043\n",
      "Epoch 4, Batch 3400, Loss: 3.047\n",
      "Epoch 4, Batch 3500, Loss: 3.038\n",
      "Epoch 4, Batch 3600, Loss: 3.043\n",
      "Epoch 4, Batch 3700, Loss: 3.053\n",
      "Epoch 4, Batch 3800, Loss: 3.046\n",
      "Epoch 4, Batch 3900, Loss: 3.042\n",
      "Epoch 4, Batch 4000, Loss: 3.040\n",
      "Epoch 4, Batch 4100, Loss: 3.030\n",
      "Epoch 4, Batch 4200, Loss: 3.051\n",
      "Epoch 5, Batch 100, Loss: 3.040\n",
      "Epoch 5, Batch 200, Loss: 3.039\n",
      "Epoch 5, Batch 300, Loss: 3.042\n",
      "Epoch 5, Batch 400, Loss: 3.043\n",
      "Epoch 5, Batch 500, Loss: 3.035\n",
      "Epoch 5, Batch 600, Loss: 3.043\n",
      "Epoch 5, Batch 700, Loss: 3.032\n",
      "Epoch 5, Batch 800, Loss: 3.035\n",
      "Epoch 5, Batch 900, Loss: 3.036\n",
      "Epoch 5, Batch 1000, Loss: 3.037\n",
      "Epoch 5, Batch 1100, Loss: 3.027\n",
      "Epoch 5, Batch 1200, Loss: 3.028\n",
      "Epoch 5, Batch 1300, Loss: 3.026\n",
      "Epoch 5, Batch 1400, Loss: 3.041\n",
      "Epoch 5, Batch 1500, Loss: 3.035\n",
      "Epoch 5, Batch 1600, Loss: 3.025\n",
      "Epoch 5, Batch 1700, Loss: 3.035\n",
      "Epoch 5, Batch 1800, Loss: 3.025\n",
      "Epoch 5, Batch 1900, Loss: 3.031\n",
      "Epoch 5, Batch 2000, Loss: 3.012\n",
      "Epoch 5, Batch 2100, Loss: 3.028\n",
      "Epoch 5, Batch 2200, Loss: 3.026\n",
      "Epoch 5, Batch 2300, Loss: 3.045\n",
      "Epoch 5, Batch 2400, Loss: 3.028\n",
      "Epoch 5, Batch 2500, Loss: 3.026\n",
      "Epoch 5, Batch 2600, Loss: 3.020\n",
      "Epoch 5, Batch 2700, Loss: 3.016\n",
      "Epoch 5, Batch 2800, Loss: 3.025\n",
      "Epoch 5, Batch 2900, Loss: 3.017\n",
      "Epoch 5, Batch 3000, Loss: 3.019\n",
      "Epoch 5, Batch 3100, Loss: 3.014\n",
      "Epoch 5, Batch 3200, Loss: 3.019\n",
      "Epoch 5, Batch 3300, Loss: 3.022\n",
      "Epoch 5, Batch 3400, Loss: 3.019\n",
      "Epoch 5, Batch 3500, Loss: 3.011\n",
      "Epoch 5, Batch 3600, Loss: 3.034\n",
      "Epoch 5, Batch 3700, Loss: 3.014\n",
      "Epoch 5, Batch 3800, Loss: 3.011\n",
      "Epoch 5, Batch 3900, Loss: 3.000\n",
      "Epoch 5, Batch 4000, Loss: 3.011\n",
      "Epoch 5, Batch 4100, Loss: 3.006\n",
      "Epoch 5, Batch 4200, Loss: 3.013\n",
      "Epoch 6, Batch 100, Loss: 3.010\n",
      "Epoch 6, Batch 200, Loss: 3.010\n",
      "Epoch 6, Batch 300, Loss: 3.014\n",
      "Epoch 6, Batch 400, Loss: 3.000\n",
      "Epoch 6, Batch 500, Loss: 3.001\n",
      "Epoch 6, Batch 600, Loss: 2.991\n",
      "Epoch 6, Batch 700, Loss: 3.011\n",
      "Epoch 6, Batch 800, Loss: 3.004\n",
      "Epoch 6, Batch 900, Loss: 3.012\n",
      "Epoch 6, Batch 1000, Loss: 3.008\n",
      "Epoch 6, Batch 1100, Loss: 3.013\n",
      "Epoch 6, Batch 1200, Loss: 3.008\n",
      "Epoch 6, Batch 1300, Loss: 2.986\n",
      "Epoch 6, Batch 1400, Loss: 3.009\n",
      "Epoch 6, Batch 1500, Loss: 3.004\n",
      "Epoch 6, Batch 1600, Loss: 3.019\n",
      "Epoch 6, Batch 1700, Loss: 3.008\n",
      "Epoch 6, Batch 1800, Loss: 2.994\n",
      "Epoch 6, Batch 1900, Loss: 2.991\n",
      "Epoch 6, Batch 2000, Loss: 2.984\n",
      "Epoch 6, Batch 2100, Loss: 2.991\n",
      "Epoch 6, Batch 2200, Loss: 2.980\n",
      "Epoch 6, Batch 2300, Loss: 2.984\n",
      "Epoch 6, Batch 2400, Loss: 2.998\n",
      "Epoch 6, Batch 2500, Loss: 2.985\n",
      "Epoch 6, Batch 2600, Loss: 2.985\n",
      "Epoch 6, Batch 2700, Loss: 2.975\n",
      "Epoch 6, Batch 2800, Loss: 2.978\n",
      "Epoch 6, Batch 2900, Loss: 3.004\n",
      "Epoch 6, Batch 3000, Loss: 2.990\n",
      "Epoch 6, Batch 3100, Loss: 2.991\n",
      "Epoch 6, Batch 3200, Loss: 2.981\n",
      "Epoch 6, Batch 3300, Loss: 2.985\n",
      "Epoch 6, Batch 3400, Loss: 2.985\n",
      "Epoch 6, Batch 3500, Loss: 2.979\n",
      "Epoch 6, Batch 3600, Loss: 2.977\n",
      "Epoch 6, Batch 3700, Loss: 2.983\n",
      "Epoch 6, Batch 3800, Loss: 2.982\n",
      "Epoch 6, Batch 3900, Loss: 2.990\n",
      "Epoch 6, Batch 4000, Loss: 2.974\n",
      "Epoch 6, Batch 4100, Loss: 2.978\n",
      "Epoch 6, Batch 4200, Loss: 2.972\n",
      "Epoch 7, Batch 100, Loss: 2.983\n",
      "Epoch 7, Batch 200, Loss: 2.977\n",
      "Epoch 7, Batch 300, Loss: 2.965\n",
      "Epoch 7, Batch 400, Loss: 2.945\n",
      "Epoch 7, Batch 500, Loss: 2.965\n",
      "Epoch 7, Batch 600, Loss: 2.967\n",
      "Epoch 7, Batch 700, Loss: 2.975\n",
      "Epoch 7, Batch 800, Loss: 2.964\n",
      "Epoch 7, Batch 900, Loss: 2.943\n",
      "Epoch 7, Batch 1000, Loss: 2.975\n",
      "Epoch 7, Batch 1100, Loss: 2.967\n",
      "Epoch 7, Batch 1200, Loss: 2.978\n",
      "Epoch 7, Batch 1300, Loss: 2.958\n",
      "Epoch 7, Batch 1400, Loss: 2.958\n",
      "Epoch 7, Batch 1500, Loss: 2.963\n",
      "Epoch 7, Batch 1600, Loss: 2.950\n",
      "Epoch 7, Batch 1700, Loss: 2.964\n",
      "Epoch 7, Batch 1800, Loss: 2.966\n",
      "Epoch 7, Batch 1900, Loss: 2.977\n",
      "Epoch 7, Batch 2000, Loss: 2.959\n",
      "Epoch 7, Batch 2100, Loss: 2.964\n",
      "Epoch 7, Batch 2200, Loss: 2.953\n",
      "Epoch 7, Batch 2300, Loss: 2.963\n",
      "Epoch 7, Batch 2400, Loss: 2.955\n",
      "Epoch 7, Batch 2500, Loss: 2.958\n",
      "Epoch 7, Batch 2600, Loss: 2.957\n",
      "Epoch 7, Batch 2700, Loss: 2.965\n",
      "Epoch 7, Batch 2800, Loss: 2.938\n",
      "Epoch 7, Batch 2900, Loss: 2.948\n",
      "Epoch 7, Batch 3000, Loss: 2.953\n",
      "Epoch 7, Batch 3100, Loss: 2.953\n",
      "Epoch 7, Batch 3200, Loss: 2.964\n",
      "Epoch 7, Batch 3300, Loss: 2.962\n",
      "Epoch 7, Batch 3400, Loss: 2.954\n",
      "Epoch 7, Batch 3500, Loss: 2.953\n",
      "Epoch 7, Batch 3600, Loss: 2.945\n",
      "Epoch 7, Batch 3700, Loss: 2.943\n",
      "Epoch 7, Batch 3800, Loss: 2.953\n",
      "Epoch 7, Batch 3900, Loss: 2.947\n",
      "Epoch 7, Batch 4000, Loss: 2.967\n",
      "Epoch 7, Batch 4100, Loss: 2.946\n",
      "Epoch 7, Batch 4200, Loss: 2.943\n",
      "Epoch 8, Batch 100, Loss: 2.948\n",
      "Epoch 8, Batch 200, Loss: 2.942\n",
      "Epoch 8, Batch 300, Loss: 2.929\n",
      "Epoch 8, Batch 400, Loss: 2.955\n",
      "Epoch 8, Batch 500, Loss: 2.947\n",
      "Epoch 8, Batch 600, Loss: 2.944\n",
      "Epoch 8, Batch 700, Loss: 2.950\n",
      "Epoch 8, Batch 800, Loss: 2.949\n",
      "Epoch 8, Batch 900, Loss: 2.940\n",
      "Epoch 8, Batch 1000, Loss: 2.944\n",
      "Epoch 8, Batch 1100, Loss: 2.914\n",
      "Epoch 8, Batch 1200, Loss: 2.928\n",
      "Epoch 8, Batch 1300, Loss: 2.954\n",
      "Epoch 8, Batch 1400, Loss: 2.949\n",
      "Epoch 8, Batch 1500, Loss: 2.937\n",
      "Epoch 8, Batch 1600, Loss: 2.952\n",
      "Epoch 8, Batch 1700, Loss: 2.953\n",
      "Epoch 8, Batch 1800, Loss: 2.940\n",
      "Epoch 8, Batch 1900, Loss: 2.933\n",
      "Epoch 8, Batch 2000, Loss: 2.933\n",
      "Epoch 8, Batch 2100, Loss: 2.933\n",
      "Epoch 8, Batch 2200, Loss: 2.934\n",
      "Epoch 8, Batch 2300, Loss: 2.946\n",
      "Epoch 8, Batch 2400, Loss: 2.934\n",
      "Epoch 8, Batch 2500, Loss: 2.949\n",
      "Epoch 8, Batch 2600, Loss: 2.930\n",
      "Epoch 8, Batch 2700, Loss: 2.924\n",
      "Epoch 8, Batch 2800, Loss: 2.930\n",
      "Epoch 8, Batch 2900, Loss: 2.924\n",
      "Epoch 8, Batch 3000, Loss: 2.933\n",
      "Epoch 8, Batch 3100, Loss: 2.924\n",
      "Epoch 8, Batch 3200, Loss: 2.917\n",
      "Epoch 8, Batch 3300, Loss: 2.921\n",
      "Epoch 8, Batch 3400, Loss: 2.926\n",
      "Epoch 8, Batch 3500, Loss: 2.936\n",
      "Epoch 8, Batch 3600, Loss: 2.925\n",
      "Epoch 8, Batch 3700, Loss: 2.928\n",
      "Epoch 8, Batch 3800, Loss: 2.922\n",
      "Epoch 8, Batch 3900, Loss: 2.920\n",
      "Epoch 8, Batch 4000, Loss: 2.922\n",
      "Epoch 8, Batch 4100, Loss: 2.929\n",
      "Epoch 8, Batch 4200, Loss: 2.932\n",
      "Epoch 9, Batch 100, Loss: 2.921\n",
      "Epoch 9, Batch 200, Loss: 2.906\n",
      "Epoch 9, Batch 300, Loss: 2.923\n",
      "Epoch 9, Batch 400, Loss: 2.910\n",
      "Epoch 9, Batch 500, Loss: 2.914\n",
      "Epoch 9, Batch 600, Loss: 2.910\n",
      "Epoch 9, Batch 700, Loss: 2.918\n",
      "Epoch 9, Batch 800, Loss: 2.912\n",
      "Epoch 9, Batch 900, Loss: 2.908\n",
      "Epoch 9, Batch 1000, Loss: 2.921\n",
      "Epoch 9, Batch 1100, Loss: 2.899\n",
      "Epoch 9, Batch 1200, Loss: 2.910\n",
      "Epoch 9, Batch 1300, Loss: 2.910\n",
      "Epoch 9, Batch 1400, Loss: 2.905\n",
      "Epoch 9, Batch 1500, Loss: 2.920\n",
      "Epoch 9, Batch 1600, Loss: 2.890\n",
      "Epoch 9, Batch 1700, Loss: 2.910\n",
      "Epoch 9, Batch 1800, Loss: 2.911\n",
      "Epoch 9, Batch 1900, Loss: 2.903\n",
      "Epoch 9, Batch 2000, Loss: 2.893\n",
      "Epoch 9, Batch 2100, Loss: 2.902\n",
      "Epoch 9, Batch 2200, Loss: 2.907\n",
      "Epoch 9, Batch 2300, Loss: 2.904\n",
      "Epoch 9, Batch 2400, Loss: 2.907\n",
      "Epoch 9, Batch 2500, Loss: 2.884\n",
      "Epoch 9, Batch 2600, Loss: 2.900\n",
      "Epoch 9, Batch 2700, Loss: 2.879\n",
      "Epoch 9, Batch 2800, Loss: 2.899\n",
      "Epoch 9, Batch 2900, Loss: 2.881\n",
      "Epoch 9, Batch 3000, Loss: 2.881\n",
      "Epoch 9, Batch 3100, Loss: 2.890\n",
      "Epoch 9, Batch 3200, Loss: 2.897\n",
      "Epoch 9, Batch 3300, Loss: 2.907\n",
      "Epoch 9, Batch 3400, Loss: 2.886\n",
      "Epoch 9, Batch 3500, Loss: 2.890\n",
      "Epoch 9, Batch 3600, Loss: 2.899\n",
      "Epoch 9, Batch 3700, Loss: 2.873\n",
      "Epoch 9, Batch 3800, Loss: 2.902\n",
      "Epoch 9, Batch 3900, Loss: 2.875\n",
      "Epoch 9, Batch 4000, Loss: 2.889\n",
      "Epoch 9, Batch 4100, Loss: 2.883\n",
      "Epoch 9, Batch 4200, Loss: 2.884\n",
      "Epoch 10, Batch 100, Loss: 2.886\n",
      "Epoch 10, Batch 200, Loss: 2.889\n",
      "Epoch 10, Batch 300, Loss: 2.880\n",
      "Epoch 10, Batch 400, Loss: 2.898\n",
      "Epoch 10, Batch 500, Loss: 2.896\n",
      "Epoch 10, Batch 600, Loss: 2.889\n",
      "Epoch 10, Batch 700, Loss: 2.887\n",
      "Epoch 10, Batch 800, Loss: 2.892\n",
      "Epoch 10, Batch 900, Loss: 2.876\n",
      "Epoch 10, Batch 1000, Loss: 2.878\n",
      "Epoch 10, Batch 1100, Loss: 2.878\n",
      "Epoch 10, Batch 1200, Loss: 2.886\n",
      "Epoch 10, Batch 1300, Loss: 2.890\n",
      "Epoch 10, Batch 1400, Loss: 2.881\n",
      "Epoch 10, Batch 1500, Loss: 2.891\n",
      "Epoch 10, Batch 1600, Loss: 2.876\n",
      "Epoch 10, Batch 1700, Loss: 2.875\n",
      "Epoch 10, Batch 1800, Loss: 2.881\n",
      "Epoch 10, Batch 1900, Loss: 2.871\n",
      "Epoch 10, Batch 2000, Loss: 2.864\n",
      "Epoch 10, Batch 2100, Loss: 2.881\n",
      "Epoch 10, Batch 2200, Loss: 2.876\n",
      "Epoch 10, Batch 2300, Loss: 2.892\n",
      "Epoch 10, Batch 2400, Loss: 2.896\n",
      "Epoch 10, Batch 2500, Loss: 2.867\n",
      "Epoch 10, Batch 2600, Loss: 2.878\n",
      "Epoch 10, Batch 2700, Loss: 2.852\n",
      "Epoch 10, Batch 2800, Loss: 2.881\n",
      "Epoch 10, Batch 2900, Loss: 2.890\n",
      "Epoch 10, Batch 3000, Loss: 2.854\n",
      "Epoch 10, Batch 3100, Loss: 2.875\n",
      "Epoch 10, Batch 3200, Loss: 2.877\n",
      "Epoch 10, Batch 3300, Loss: 2.875\n",
      "Epoch 10, Batch 3400, Loss: 2.866\n",
      "Epoch 10, Batch 3500, Loss: 2.900\n",
      "Epoch 10, Batch 3600, Loss: 2.890\n",
      "Epoch 10, Batch 3700, Loss: 2.879\n",
      "Epoch 10, Batch 3800, Loss: 2.885\n",
      "Epoch 10, Batch 3900, Loss: 2.881\n",
      "Epoch 10, Batch 4000, Loss: 2.866\n",
      "Epoch 10, Batch 4100, Loss: 2.874\n",
      "Epoch 10, Batch 4200, Loss: 2.880\n",
      "Finished Training\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type ChessPolicyModel is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 56\u001b[0m\n\u001b[0;32m     53\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), model_path)\n\u001b[0;32m     55\u001b[0m model_info_dict \u001b[38;5;241m=\u001b[39m get_model_info_dict(model_path, model, train_data_path)\n\u001b[1;32m---> 56\u001b[0m \u001b[43msave_info_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_info_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDeepLearningAI\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m model_loss_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels_loss/dl_model(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnow_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(model_loss_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[1;32mIn[1], line 93\u001b[0m, in \u001b[0;36msave_info_dict\u001b[1;34m(info_dict, model_type)\u001b[0m\n\u001b[0;32m     90\u001b[0m model_score[model_type]\u001b[38;5;241m.\u001b[39mappend(info_dict)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_score.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mq:\\ProgramData\\miniforge3\\envs\\toolenv\\lib\\json\\__init__.py:179\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    173\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(skipkeys\u001b[38;5;241m=\u001b[39mskipkeys, ensure_ascii\u001b[38;5;241m=\u001b[39mensure_ascii,\n\u001b[0;32m    174\u001b[0m         check_circular\u001b[38;5;241m=\u001b[39mcheck_circular, allow_nan\u001b[38;5;241m=\u001b[39mallow_nan, indent\u001b[38;5;241m=\u001b[39mindent,\n\u001b[0;32m    175\u001b[0m         separators\u001b[38;5;241m=\u001b[39mseparators,\n\u001b[0;32m    176\u001b[0m         default\u001b[38;5;241m=\u001b[39mdefault, sort_keys\u001b[38;5;241m=\u001b[39msort_keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\u001b[38;5;241m.\u001b[39miterencode(obj)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m    180\u001b[0m     fp\u001b[38;5;241m.\u001b[39mwrite(chunk)\n",
      "File \u001b[1;32mq:\\ProgramData\\miniforge3\\envs\\toolenv\\lib\\json\\encoder.py:431\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m--> 431\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mq:\\ProgramData\\miniforge3\\envs\\toolenv\\lib\\json\\encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[1;34m(dct, _current_indent_level)\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    404\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[1;32m--> 405\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    407\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mq:\\ProgramData\\miniforge3\\envs\\toolenv\\lib\\json\\encoder.py:325\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_list\u001b[1;34m(lst, _current_indent_level)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    324\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[1;32m--> 325\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    327\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mq:\\ProgramData\\miniforge3\\envs\\toolenv\\lib\\json\\encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[1;34m(dct, _current_indent_level)\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    404\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[1;32m--> 405\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    407\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mq:\\ProgramData\\miniforge3\\envs\\toolenv\\lib\\json\\encoder.py:438\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCircular reference detected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    437\u001b[0m     markers[markerid] \u001b[38;5;241m=\u001b[39m o\n\u001b[1;32m--> 438\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mq:\\ProgramData\\miniforge3\\envs\\toolenv\\lib\\json\\encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m \n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    180\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type ChessPolicyModel is not JSON serializable"
     ]
    }
   ],
   "source": [
    "from time import strftime, localtime\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from ai.deeplearning import ChessPolicyModel\n",
    "\n",
    "\n",
    "# 设置CuDNN选项\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ChessPolicyModel().to(device) # 初始化模型，并将其移动到GPU上\n",
    "criterion = nn.CrossEntropyLoss() # 定义交叉熵损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # 定义Adam优化器\n",
    "\n",
    "train_log_text = []\n",
    "# 训练循环\n",
    "num_epochs = 10  # 训练10个epoch\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        inputs, labels = data\n",
    "        # 调整输入的维度，并将其移动到GPU上\n",
    "        # inputs 的原始形状是 (batch_size, height, width, channels)，也就是 (32, 5, 5, 3)\n",
    "        # inputs.permute(0, 3, 1, 2) 会将 inputs 的维度从 (32, 5, 5, 3) 转换为 (32, 3, 5, 5)\n",
    "        inputs = inputs.permute(0, 3, 1, 2).to(device)  # (batch_size, channels, height, width)\n",
    "        labels = labels.to(device).to(torch.int64)\n",
    "\n",
    "        # 清零梯度\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(inputs)\n",
    "        # 计算损失\n",
    "        loss = criterion(outputs, labels)\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        # 更新参数\n",
    "        optimizer.step()\n",
    "\n",
    "        # 累积损失\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # 每100个batch打印一次loss\n",
    "            log_text = f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100:.3f}'\n",
    "            print(log_text)\n",
    "            train_log_text.append(log_text)\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "data_size = len(dataset)\n",
    "now_time = strftime(\"%m-%d-%H-%M\", localtime())\n",
    "model_path = f'models/dl_model({now_time})({data_size}).pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "model_info_dict = get_model_info_dict(model_path, train_data_path, model)\n",
    "save_info_dict(model_info_dict, \"DeepLearningAI\")\n",
    "\n",
    "model_loss_path = f'models_loss/dl_model({now_time})({data_size}).txt'\n",
    "with open(model_loss_path, 'w') as f:\n",
    "    f.write('\\n'.join(train_log_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1             [-1, 32, 5, 5]             896\n",
      "            Conv2d-2             [-1, 64, 5, 5]          18,496\n",
      "            Conv2d-3            [-1, 128, 5, 5]          73,856\n",
      "            Conv2d-4            [-1, 256, 5, 5]         295,168\n",
      "            Linear-5                  [-1, 512]       3,277,312\n",
      "           Dropout-6                  [-1, 512]               0\n",
      "            Linear-7                   [-1, 25]          12,825\n",
      "================================================================\n",
      "Total params: 3,678,553\n",
      "Trainable params: 3,678,553\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.10\n",
      "Params size (MB): 14.03\n",
      "Estimated Total Size (MB): 14.13\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, input_size=(3, 5, 5))  # 输入模型和输入tensor尺寸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 确定项目根目录（假设当前工作目录是项目的根目录）\n",
    "project_root = os.path.abspath(\"..\")\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "from tqdm import tqdm\n",
    "from ai import MinimaxAI, MCTSAI\n",
    "from ai.test_ai import get_model_score_by_mcts, ai_battle\n",
    "from ai.deeplearning import DeepLearningAI\n",
    "from game.chess_game import ChessGame\n",
    "\n",
    "\n",
    "def get_best_c_param(game_state):\n",
    "    best_c_param = 0.0\n",
    "    c_param_dict = dict()\n",
    "    best_mcts = MCTSAI(100, c_param=best_c_param, policy_net=policy_model, complate_mode=False)\n",
    "\n",
    "    for param in tqdm(range(0, 11, 1)):\n",
    "        win = 0\n",
    "        test_mcts = MCTSAI(100, c_param=param/10, policy_net=policy_model, complate_mode=False)\n",
    "\n",
    "        for _ in range(100):\n",
    "            test_game = ChessGame(*game_state)\n",
    "            test_game.init_history()\n",
    "            over_game = ai_battle(best_mcts, test_mcts, test_game, display=False)\n",
    "            winner = over_game.who_is_winner()\n",
    "            if winner == 1:\n",
    "                win += 1\n",
    "            elif winner == 0:\n",
    "                win += 0.5\n",
    "\n",
    "        c_param_dict[f\"{best_c_param} : {param/10}\"] = win\n",
    "        if win < 50:\n",
    "            best_c_param = param/10\n",
    "            best_mcts = MCTSAI(100, c_param=best_c_param, complate_mode=False)\n",
    "        \n",
    "    return best_c_param, c_param_dict\n",
    "\n",
    "# 与其他AI算法进行对战\n",
    "game_state = ((5,5), 2)\n",
    "\n",
    "# policy_model = DeepLearningAI('models/dl_model(06-22-21-18)(136090)(32-64-128-256).pth', complate_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [44:03<00:00, 240.33s/it] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0,\n",
       " {'0 : 0.0': 47.0,\n",
       "  '0.0 : 0.1': 70.0,\n",
       "  '0.0 : 0.2': 60.0,\n",
       "  '0.0 : 0.3': 63.5,\n",
       "  '0.0 : 0.4': 40.0,\n",
       "  '0.4 : 0.5': 49.5,\n",
       "  '0.5 : 0.6': 55.0,\n",
       "  '0.5 : 0.7': 55.0,\n",
       "  '0.5 : 0.8': 53.5,\n",
       "  '0.5 : 0.9': 53.0,\n",
       "  '0.5 : 1.0': 47.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_best_c_param(game_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/999 [00:41<1:08:30,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, {10: 0.73, 20: 0.72, 30: 0.66, 40: 0.685, 50: 0.585, 60: 0.575, 70: 0.59, 80: 0.48, 90: 0.55, 100: 0.48, 110: 0.525})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(get_model_score_by_mcts(MCTSAI(100, 0.9, complate_mode=False), game_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 8/999 [26:49<55:23:30, 201.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, {10: 0.735, 20: 0.69, 30: 0.67, 40: 0.565, 50: 0.545, 60: 0.56, 70: 0.555, 80: 0.5, 90: 0.49})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "policy_model = DeepLearningAI('models/dl_model(06-22-21-18)(136090)(32-64-128-256).pth', complate_mode=False)\n",
    "mcts_model = MCTSAI(100, c_param=0.5, policy_net=policy_model, complate_mode=False)\n",
    "\n",
    "print(get_model_score_by_mcts(mcts_model, game_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/999 [00:03<55:17,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, {10: 0.63, 20: 0.54})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(get_model_score_by_mcts(DeepLearningAI(model_path, complate_mode=False), game_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deeplearning_model = DeepLearningAI(r\"models\\chess_ai_model(06-15-17)(15475).pth\") \n",
    "\n",
    "len(deeplearning_model.model.conv1.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 深度学习AI\n",
    "deep_learning_dict = {\n",
    "                      # {10: 0.7, 20: 0.68, 30: 0.47}\n",
    "                      # \n",
    "                      \"models/chess_ai_model(06-13-19).pth\": 20, # 未知\n",
    "                      # {10: 0.7, 20: 0.51, 30: 0.43}\n",
    "                      # \n",
    "                      \"models/chess_ai_model(06-15-17)(1506).pth\": 20, # MCTSAI(1000, flag=True) 训练100轮\n",
    "                      # {10: 0.75, 20: 0.65, 30: 0.47}\n",
    "                      # \n",
    "                      'models/chess_ai_model(06-15-17)(1471).pth': 20, # MCTSAI(10000, flag=True) 训练100轮\n",
    "                      # {10: 0.63, 20: 0.41}\n",
    "                      'models/chess_ai_model(06-15-17)(936).pth': 10, # MCTSAI(10000, flag=True) 训练100轮并去重\n",
    "                      # {10: 0.82, 20: 0.76, 30: 0.65, 40: 0.57, 50: 0.56, 60: 0.44}\n",
    "                      # \n",
    "                      'models/chess_ai_model(06-15-17)(15475).pth': 50, # MCTSAI(1000, flag=True) 训练1000轮\n",
    "                      # {10: 0.84, 20: 0.58, 30: 0.57, 40: 0.61, 50: 0.56, 60: 0.41}\n",
    "                      'models/chess_ai_model(06-15-17)(9598).pth': 50, # MCTSAI(1000, flag=True) 训练1000轮并去重\n",
    "                      # {10: 0.32}\n",
    "                      \"models/chess_ai_model(06-16-15)(1400).pth\": 0, # MCTSAI(10000, flag=False) 训练100轮(没标错, 真是0分)\n",
    "                      # {10: 0.55, 20: 0.48}\n",
    "                      'models/chess_ai_model(06-16-20)(1542).pth': 10, # MCTSAI(1000, flag=False) 训练100轮\n",
    "\n",
    "                      # {10: 0.82, 20: 0.73, 30: 0.67, 40: 0.52, 50: 0.46}\n",
    "                      # \n",
    "                      'models/chess_ai_model(06-17-12)(15475).pth': 40, # MCTSAI(1000, flag=True) 训练1000轮, 使用三层32-64-128卷积层, 128-256-25全连接层\n",
    "                      # {10: 0.72, 20: 0.39}\n",
    "                      'models/chess_ai_model(06-17-12)(1506).pth': 10, # MCTSAI(1000, flag=True) 训练100轮, 使用三层32-64-128卷积层, 128-256-25全连接层\n",
    "                      \n",
    "                      # {10: 0.9, 20: 0.84, 30: 0.72, 40: 0.6, 50: 0.48}\n",
    "                      # {10: 0.88, 20: 0.83, 30: 0.69, 40: 0.68, 50: 0.57, 60: 0.65, 70: 0.61, 80: 0.63, 90: 0.49}\n",
    "                      # \n",
    "                      'models/chess_ai_model(06-17-13)(15475).pth': 80, # MCTSAI(1000, flag=True) 训练1000轮, 使用三层16-32-64卷积层, 64-128-25全连接层\n",
    "                      }\n",
    "\n",
    "                    \n",
    "# deep_learning_ai_0 = DeepLearningAI(\"models/chess_ai_model(06-15-17)(936).pth\") \n",
    "\n",
    "# 测试AI\n",
    "test_minimax = MinimaxAI(6) # score: 50\n",
    "test_mcts_0 = MCTSAI(1000) # \n",
    "test_mcts_1 = MCTSAI(100, flag=True) # score: 50\n",
    "test_mcts_2 = MCTSAI(100, flag=False) # score: 80\n",
    "test_mcts_3 = MCTSAI(60, flag=True) # 50, {10: 0.82, 20: 0.76, 30: 0.53, 40: 0.57, 50: 0.51, 60: 0.32}\n",
    "test_mcts_4 = MCTSAI(60, flag=False) # 110, {10: 0.86, 20: 0.86, 30: 0.77, 40: 0.63, 50: 0.58, 60: 0.63, 70: 0.58, 80: 0.52, 90: 0.51, 100: 0.5, 110: 0.52, 120: 0.45}\n",
    "test_mcts_5 = MCTSAI(10, flag=True) # 0, {10: 0.45}\n",
    "test_mcts_6 = MCTSAI(10, flag=False) # 50, {10: 0.76, 20: 0.69, 30: 0.57, 40: 0.56, 50: 0.5, 60: 0.4}\n",
    "test_mcts_7 = MCTSAI(500, flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chessenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
