{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA RTX A5000\n",
      "tensor([[0.4938, 0.5651, 0.5798],\n",
      "        [0.5265, 0.5529, 0.2071],\n",
      "        [0.1274, 0.2573, 0.5208]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "flag = torch.cuda.is_available()\n",
    "print(flag)\n",
    "\n",
    "if flag:\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(torch.rand(3,3).cuda()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 确定项目根目录（假设当前工作目录是项目的根目录）\n",
    "project_root = os.path.abspath(\"..\")\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from ai import ai_battle, MinimaxAI, MCTSAI\n",
    "from game.chess_game import ChessGame\n",
    "from CelestialVault.instances import ExampleThreadManager\n",
    "from time import strftime, localtime\n",
    "\n",
    "\n",
    "class TrainDataThread(ExampleThreadManager):\n",
    "    def get_args(self, obj: object):\n",
    "        train_game = ChessGame((5, 5), 2)\n",
    "        train_game.init_cfunc()\n",
    "        train_game.init_history()\n",
    "        return (mcts_ai_0, mcts_ai_0, train_game, False)\n",
    "    \n",
    "    def process_result(self):\n",
    "        all_training_data = []\n",
    "        result_dict = self.get_result_dict()\n",
    "        for over_game in result_dict.values():\n",
    "            history_board = over_game.history_board\n",
    "            history_move = over_game.history_move\n",
    "            for step in range(over_game.max_step-1):\n",
    "                board = self.process_board(history_board[step], step)\n",
    "                # if (board, history_move[step+1]) in all_training_data: # 这样效果并不好\n",
    "                #     continue\n",
    "                all_training_data.append((board, history_move[step+1]))\n",
    "        return all_training_data\n",
    "    \n",
    "    def process_board(self, chess_board, step):\n",
    "        color = 1 if step % 2 == 0 else -1\n",
    "        color_channel = np.full((5, 5, 1), color)\n",
    "\n",
    "        processed_board = np.concatenate((chess_board, color_channel), axis=2)\n",
    "        \n",
    "        for row in processed_board:\n",
    "            for cell in row:\n",
    "                if cell[0] == float(\"inf\"):\n",
    "                    cell[0] = 5\n",
    "        return processed_board\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "def save_data(data):\n",
    "    data_size = len(data)\n",
    "    now_time = strftime(\"%m-%d-%H\", localtime())\n",
    "    pickle.dump(data, open(f\"train_data/all_training_data({now_time})({data_size}).pkl\", \"wb\"))\n",
    "\n",
    "def train_data(train_num):\n",
    "    train_data_threader.start(range(train_num), \"serial\")\n",
    "    train_data_threader.handle_error()\n",
    "    all_training_data = train_data_threader.process_result()\n",
    "\n",
    "    save_data(all_training_data)\n",
    "    \n",
    "    return all_training_data\n",
    "\n",
    "minimax_ai = MinimaxAI(5)\n",
    "mcts_ai_0 = MCTSAI(1000, complate_mode=False)\n",
    "mcts_ai_1 = MCTSAI(50000, complate_mode=False)\n",
    "\n",
    "train_data_threader = TrainDataThread(\n",
    "            ai_battle,\n",
    "            thread_num=200,\n",
    "            tqdm_desc='trainDataProcess',\n",
    "            show_progress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "trainDataProcess: 100%|██████████| 200/200 [01:25<00:00,  2.33it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:25<00:00,  2.34it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:27<00:00,  2.28it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:26<00:00,  2.31it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:17<00:00,  2.60it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:17<00:00,  2.58it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:20<00:00,  2.48it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:18<00:00,  2.54it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:19<00:00,  2.50it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:19<00:00,  2.51it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:18<00:00,  2.54it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:17<00:00,  2.58it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:18<00:00,  2.53it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:17<00:00,  2.59it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:16<00:00,  2.62it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:18<00:00,  2.56it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:16<00:00,  2.62it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:17<00:00,  2.58it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:18<00:00,  2.54it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:16<00:00,  2.63it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:17<00:00,  2.58it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:18<00:00,  2.54it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:17<00:00,  2.59it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:17<00:00,  2.59it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:17<00:00,  2.57it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:18<00:00,  2.55it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:18<00:00,  2.54it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:18<00:00,  2.55it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:17<00:00,  2.59it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:19<00:00,  2.53it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:19<00:00,  2.52it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:19<00:00,  2.53it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:18<00:00,  2.55it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:17<00:00,  2.59it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:18<00:00,  2.56it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:19<00:00,  2.53it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:19<00:00,  2.53it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:17<00:00,  2.59it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:18<00:00,  2.54it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:16<00:00,  2.60it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:17<00:00,  2.56it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:17<00:00,  2.59it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:17<00:00,  2.58it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:17<00:00,  2.60it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:18<00:00,  2.56it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:16<00:00,  2.62it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:19<00:00,  2.52it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:16<00:00,  2.61it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:17<00:00,  2.60it/s]\n",
      "trainDataProcess: 100%|██████████| 200/200 [01:18<00:00,  2.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "136090"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_training_data = []\n",
    "for _ in range(1):\n",
    "    all_training_data += train_data(10000)\n",
    "\n",
    "len(all_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.]],\n",
       " \n",
       "        [[0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.]],\n",
       " \n",
       "        [[0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.]],\n",
       " \n",
       "        [[0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.]],\n",
       " \n",
       "        [[0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 1.]]]),\n",
       " array([1, 2]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_training_data = load_data(r\"train_data\\train_data(06-22-17)(136090).pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136090"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class ChessDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        board_state, move = self.data[idx]\n",
    "        board_state = torch.tensor(board_state, dtype=torch.float32)\n",
    "        move = move[0] * 5 + move[1]\n",
    "        return board_state, move\n",
    "\n",
    "dataset = ChessDataset(all_training_data)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100, Loss: 3.136\n",
      "Epoch 1, Batch 200, Loss: 3.050\n",
      "Epoch 1, Batch 300, Loss: 3.003\n",
      "Epoch 1, Batch 400, Loss: 2.915\n",
      "Epoch 1, Batch 500, Loss: 2.831\n",
      "Epoch 1, Batch 600, Loss: 2.754\n",
      "Epoch 1, Batch 700, Loss: 2.690\n",
      "Epoch 1, Batch 800, Loss: 2.654\n",
      "Epoch 1, Batch 900, Loss: 2.587\n",
      "Epoch 1, Batch 1000, Loss: 2.561\n",
      "Epoch 1, Batch 1100, Loss: 2.505\n",
      "Epoch 1, Batch 1200, Loss: 2.490\n",
      "Epoch 1, Batch 1300, Loss: 2.403\n",
      "Epoch 1, Batch 1400, Loss: 2.303\n",
      "Epoch 1, Batch 1500, Loss: 2.285\n",
      "Epoch 1, Batch 1600, Loss: 2.233\n",
      "Epoch 1, Batch 1700, Loss: 2.215\n",
      "Epoch 1, Batch 1800, Loss: 2.168\n",
      "Epoch 1, Batch 1900, Loss: 2.125\n",
      "Epoch 1, Batch 2000, Loss: 2.086\n",
      "Epoch 1, Batch 2100, Loss: 2.052\n",
      "Epoch 1, Batch 2200, Loss: 2.030\n",
      "Epoch 1, Batch 2300, Loss: 1.962\n",
      "Epoch 1, Batch 2400, Loss: 1.884\n",
      "Epoch 1, Batch 2500, Loss: 1.872\n",
      "Epoch 1, Batch 2600, Loss: 1.867\n",
      "Epoch 1, Batch 2700, Loss: 1.837\n",
      "Epoch 1, Batch 2800, Loss: 1.798\n",
      "Epoch 1, Batch 2900, Loss: 1.795\n",
      "Epoch 1, Batch 3000, Loss: 1.753\n",
      "Epoch 1, Batch 3100, Loss: 1.796\n",
      "Epoch 1, Batch 3200, Loss: 1.772\n",
      "Epoch 1, Batch 3300, Loss: 1.719\n",
      "Epoch 1, Batch 3400, Loss: 1.727\n",
      "Epoch 1, Batch 3500, Loss: 1.720\n",
      "Epoch 1, Batch 3600, Loss: 1.725\n",
      "Epoch 1, Batch 3700, Loss: 1.713\n",
      "Epoch 1, Batch 3800, Loss: 1.671\n",
      "Epoch 1, Batch 3900, Loss: 1.675\n",
      "Epoch 1, Batch 4000, Loss: 1.668\n",
      "Epoch 1, Batch 4100, Loss: 1.631\n",
      "Epoch 1, Batch 4200, Loss: 1.631\n",
      "Epoch 2, Batch 100, Loss: 1.589\n",
      "Epoch 2, Batch 200, Loss: 1.568\n",
      "Epoch 2, Batch 300, Loss: 1.612\n",
      "Epoch 2, Batch 400, Loss: 1.599\n",
      "Epoch 2, Batch 500, Loss: 1.579\n",
      "Epoch 2, Batch 600, Loss: 1.559\n",
      "Epoch 2, Batch 700, Loss: 1.622\n",
      "Epoch 2, Batch 800, Loss: 1.561\n",
      "Epoch 2, Batch 900, Loss: 1.588\n",
      "Epoch 2, Batch 1000, Loss: 1.578\n",
      "Epoch 2, Batch 1100, Loss: 1.560\n",
      "Epoch 2, Batch 1200, Loss: 1.511\n",
      "Epoch 2, Batch 1300, Loss: 1.558\n",
      "Epoch 2, Batch 1400, Loss: 1.603\n",
      "Epoch 2, Batch 1500, Loss: 1.575\n",
      "Epoch 2, Batch 1600, Loss: 1.535\n",
      "Epoch 2, Batch 1700, Loss: 1.539\n",
      "Epoch 2, Batch 1800, Loss: 1.574\n",
      "Epoch 2, Batch 1900, Loss: 1.566\n",
      "Epoch 2, Batch 2000, Loss: 1.519\n",
      "Epoch 2, Batch 2100, Loss: 1.573\n",
      "Epoch 2, Batch 2200, Loss: 1.551\n",
      "Epoch 2, Batch 2300, Loss: 1.513\n",
      "Epoch 2, Batch 2400, Loss: 1.562\n",
      "Epoch 2, Batch 2500, Loss: 1.541\n",
      "Epoch 2, Batch 2600, Loss: 1.507\n",
      "Epoch 2, Batch 2700, Loss: 1.557\n",
      "Epoch 2, Batch 2800, Loss: 1.547\n",
      "Epoch 2, Batch 2900, Loss: 1.544\n",
      "Epoch 2, Batch 3000, Loss: 1.518\n",
      "Epoch 2, Batch 3100, Loss: 1.527\n",
      "Epoch 2, Batch 3200, Loss: 1.529\n",
      "Epoch 2, Batch 3300, Loss: 1.521\n",
      "Epoch 2, Batch 3400, Loss: 1.533\n",
      "Epoch 2, Batch 3500, Loss: 1.518\n",
      "Epoch 2, Batch 3600, Loss: 1.554\n",
      "Epoch 2, Batch 3700, Loss: 1.459\n",
      "Epoch 2, Batch 3800, Loss: 1.519\n",
      "Epoch 2, Batch 3900, Loss: 1.498\n",
      "Epoch 2, Batch 4000, Loss: 1.486\n",
      "Epoch 2, Batch 4100, Loss: 1.494\n",
      "Epoch 2, Batch 4200, Loss: 1.488\n",
      "Epoch 3, Batch 100, Loss: 1.369\n",
      "Epoch 3, Batch 200, Loss: 1.420\n",
      "Epoch 3, Batch 300, Loss: 1.409\n",
      "Epoch 3, Batch 400, Loss: 1.409\n",
      "Epoch 3, Batch 500, Loss: 1.403\n",
      "Epoch 3, Batch 600, Loss: 1.411\n",
      "Epoch 3, Batch 700, Loss: 1.393\n",
      "Epoch 3, Batch 800, Loss: 1.426\n",
      "Epoch 3, Batch 900, Loss: 1.424\n",
      "Epoch 3, Batch 1000, Loss: 1.423\n",
      "Epoch 3, Batch 1100, Loss: 1.425\n",
      "Epoch 3, Batch 1200, Loss: 1.398\n",
      "Epoch 3, Batch 1300, Loss: 1.482\n",
      "Epoch 3, Batch 1400, Loss: 1.423\n",
      "Epoch 3, Batch 1500, Loss: 1.471\n",
      "Epoch 3, Batch 1600, Loss: 1.443\n",
      "Epoch 3, Batch 1700, Loss: 1.440\n",
      "Epoch 3, Batch 1800, Loss: 1.431\n",
      "Epoch 3, Batch 1900, Loss: 1.416\n",
      "Epoch 3, Batch 2000, Loss: 1.486\n",
      "Epoch 3, Batch 2100, Loss: 1.419\n",
      "Epoch 3, Batch 2200, Loss: 1.415\n",
      "Epoch 3, Batch 2300, Loss: 1.420\n",
      "Epoch 3, Batch 2400, Loss: 1.420\n",
      "Epoch 3, Batch 2500, Loss: 1.425\n",
      "Epoch 3, Batch 2600, Loss: 1.435\n",
      "Epoch 3, Batch 2700, Loss: 1.454\n",
      "Epoch 3, Batch 2800, Loss: 1.431\n",
      "Epoch 3, Batch 2900, Loss: 1.419\n",
      "Epoch 3, Batch 3000, Loss: 1.432\n",
      "Epoch 3, Batch 3100, Loss: 1.403\n",
      "Epoch 3, Batch 3200, Loss: 1.399\n",
      "Epoch 3, Batch 3300, Loss: 1.398\n",
      "Epoch 3, Batch 3400, Loss: 1.426\n",
      "Epoch 3, Batch 3500, Loss: 1.384\n",
      "Epoch 3, Batch 3600, Loss: 1.411\n",
      "Epoch 3, Batch 3700, Loss: 1.416\n",
      "Epoch 3, Batch 3800, Loss: 1.414\n",
      "Epoch 3, Batch 3900, Loss: 1.421\n",
      "Epoch 3, Batch 4000, Loss: 1.422\n",
      "Epoch 3, Batch 4100, Loss: 1.420\n",
      "Epoch 3, Batch 4200, Loss: 1.401\n",
      "Epoch 4, Batch 100, Loss: 1.310\n",
      "Epoch 4, Batch 200, Loss: 1.298\n",
      "Epoch 4, Batch 300, Loss: 1.320\n",
      "Epoch 4, Batch 400, Loss: 1.342\n",
      "Epoch 4, Batch 500, Loss: 1.321\n",
      "Epoch 4, Batch 600, Loss: 1.322\n",
      "Epoch 4, Batch 700, Loss: 1.309\n",
      "Epoch 4, Batch 800, Loss: 1.340\n",
      "Epoch 4, Batch 900, Loss: 1.338\n",
      "Epoch 4, Batch 1000, Loss: 1.339\n",
      "Epoch 4, Batch 1100, Loss: 1.351\n",
      "Epoch 4, Batch 1200, Loss: 1.327\n",
      "Epoch 4, Batch 1300, Loss: 1.352\n",
      "Epoch 4, Batch 1400, Loss: 1.334\n",
      "Epoch 4, Batch 1500, Loss: 1.308\n",
      "Epoch 4, Batch 1600, Loss: 1.333\n",
      "Epoch 4, Batch 1700, Loss: 1.332\n",
      "Epoch 4, Batch 1800, Loss: 1.359\n",
      "Epoch 4, Batch 1900, Loss: 1.370\n",
      "Epoch 4, Batch 2000, Loss: 1.334\n",
      "Epoch 4, Batch 2100, Loss: 1.400\n",
      "Epoch 4, Batch 2200, Loss: 1.324\n",
      "Epoch 4, Batch 2300, Loss: 1.348\n",
      "Epoch 4, Batch 2400, Loss: 1.327\n",
      "Epoch 4, Batch 2500, Loss: 1.354\n",
      "Epoch 4, Batch 2600, Loss: 1.363\n",
      "Epoch 4, Batch 2700, Loss: 1.337\n",
      "Epoch 4, Batch 2800, Loss: 1.330\n",
      "Epoch 4, Batch 2900, Loss: 1.353\n",
      "Epoch 4, Batch 3000, Loss: 1.346\n",
      "Epoch 4, Batch 3100, Loss: 1.351\n",
      "Epoch 4, Batch 3200, Loss: 1.347\n",
      "Epoch 4, Batch 3300, Loss: 1.358\n",
      "Epoch 4, Batch 3400, Loss: 1.363\n",
      "Epoch 4, Batch 3500, Loss: 1.332\n",
      "Epoch 4, Batch 3600, Loss: 1.340\n",
      "Epoch 4, Batch 3700, Loss: 1.353\n",
      "Epoch 4, Batch 3800, Loss: 1.337\n",
      "Epoch 4, Batch 3900, Loss: 1.369\n",
      "Epoch 4, Batch 4000, Loss: 1.352\n",
      "Epoch 4, Batch 4100, Loss: 1.338\n",
      "Epoch 4, Batch 4200, Loss: 1.309\n",
      "Epoch 5, Batch 100, Loss: 1.220\n",
      "Epoch 5, Batch 200, Loss: 1.244\n",
      "Epoch 5, Batch 300, Loss: 1.203\n",
      "Epoch 5, Batch 400, Loss: 1.204\n",
      "Epoch 5, Batch 500, Loss: 1.235\n",
      "Epoch 5, Batch 600, Loss: 1.231\n",
      "Epoch 5, Batch 700, Loss: 1.254\n",
      "Epoch 5, Batch 800, Loss: 1.226\n",
      "Epoch 5, Batch 900, Loss: 1.265\n",
      "Epoch 5, Batch 1000, Loss: 1.268\n",
      "Epoch 5, Batch 1100, Loss: 1.237\n",
      "Epoch 5, Batch 1200, Loss: 1.274\n",
      "Epoch 5, Batch 1300, Loss: 1.271\n",
      "Epoch 5, Batch 1400, Loss: 1.274\n",
      "Epoch 5, Batch 1500, Loss: 1.274\n",
      "Epoch 5, Batch 1600, Loss: 1.314\n",
      "Epoch 5, Batch 1700, Loss: 1.266\n",
      "Epoch 5, Batch 1800, Loss: 1.250\n",
      "Epoch 5, Batch 1900, Loss: 1.293\n",
      "Epoch 5, Batch 2000, Loss: 1.275\n",
      "Epoch 5, Batch 2100, Loss: 1.285\n",
      "Epoch 5, Batch 2200, Loss: 1.282\n",
      "Epoch 5, Batch 2300, Loss: 1.278\n",
      "Epoch 5, Batch 2400, Loss: 1.266\n",
      "Epoch 5, Batch 2500, Loss: 1.293\n",
      "Epoch 5, Batch 2600, Loss: 1.290\n",
      "Epoch 5, Batch 2700, Loss: 1.287\n",
      "Epoch 5, Batch 2800, Loss: 1.309\n",
      "Epoch 5, Batch 2900, Loss: 1.274\n",
      "Epoch 5, Batch 3000, Loss: 1.254\n",
      "Epoch 5, Batch 3100, Loss: 1.301\n",
      "Epoch 5, Batch 3200, Loss: 1.260\n",
      "Epoch 5, Batch 3300, Loss: 1.306\n",
      "Epoch 5, Batch 3400, Loss: 1.304\n",
      "Epoch 5, Batch 3500, Loss: 1.303\n",
      "Epoch 5, Batch 3600, Loss: 1.294\n",
      "Epoch 5, Batch 3700, Loss: 1.355\n",
      "Epoch 5, Batch 3800, Loss: 1.273\n",
      "Epoch 5, Batch 3900, Loss: 1.304\n",
      "Epoch 5, Batch 4000, Loss: 1.297\n",
      "Epoch 5, Batch 4100, Loss: 1.347\n",
      "Epoch 5, Batch 4200, Loss: 1.279\n",
      "Epoch 6, Batch 100, Loss: 1.190\n",
      "Epoch 6, Batch 200, Loss: 1.165\n",
      "Epoch 6, Batch 300, Loss: 1.168\n",
      "Epoch 6, Batch 400, Loss: 1.166\n",
      "Epoch 6, Batch 500, Loss: 1.192\n",
      "Epoch 6, Batch 600, Loss: 1.155\n",
      "Epoch 6, Batch 700, Loss: 1.163\n",
      "Epoch 6, Batch 800, Loss: 1.196\n",
      "Epoch 6, Batch 900, Loss: 1.218\n",
      "Epoch 6, Batch 1000, Loss: 1.253\n",
      "Epoch 6, Batch 1100, Loss: 1.178\n",
      "Epoch 6, Batch 1200, Loss: 1.178\n",
      "Epoch 6, Batch 1300, Loss: 1.188\n",
      "Epoch 6, Batch 1400, Loss: 1.233\n",
      "Epoch 6, Batch 1500, Loss: 1.231\n",
      "Epoch 6, Batch 1600, Loss: 1.201\n",
      "Epoch 6, Batch 1700, Loss: 1.213\n",
      "Epoch 6, Batch 1800, Loss: 1.223\n",
      "Epoch 6, Batch 1900, Loss: 1.219\n",
      "Epoch 6, Batch 2000, Loss: 1.200\n",
      "Epoch 6, Batch 2100, Loss: 1.248\n",
      "Epoch 6, Batch 2200, Loss: 1.278\n",
      "Epoch 6, Batch 2300, Loss: 1.212\n",
      "Epoch 6, Batch 2400, Loss: 1.260\n",
      "Epoch 6, Batch 2500, Loss: 1.202\n",
      "Epoch 6, Batch 2600, Loss: 1.257\n",
      "Epoch 6, Batch 2700, Loss: 1.208\n",
      "Epoch 6, Batch 2800, Loss: 1.206\n",
      "Epoch 6, Batch 2900, Loss: 1.237\n",
      "Epoch 6, Batch 3000, Loss: 1.233\n",
      "Epoch 6, Batch 3100, Loss: 1.220\n",
      "Epoch 6, Batch 3200, Loss: 1.256\n",
      "Epoch 6, Batch 3300, Loss: 1.244\n",
      "Epoch 6, Batch 3400, Loss: 1.245\n",
      "Epoch 6, Batch 3500, Loss: 1.238\n",
      "Epoch 6, Batch 3600, Loss: 1.206\n",
      "Epoch 6, Batch 3700, Loss: 1.229\n",
      "Epoch 6, Batch 3800, Loss: 1.261\n",
      "Epoch 6, Batch 3900, Loss: 1.278\n",
      "Epoch 6, Batch 4000, Loss: 1.239\n",
      "Epoch 6, Batch 4100, Loss: 1.273\n",
      "Epoch 6, Batch 4200, Loss: 1.230\n",
      "Epoch 7, Batch 100, Loss: 1.134\n",
      "Epoch 7, Batch 200, Loss: 1.105\n",
      "Epoch 7, Batch 300, Loss: 1.109\n",
      "Epoch 7, Batch 400, Loss: 1.105\n",
      "Epoch 7, Batch 500, Loss: 1.148\n",
      "Epoch 7, Batch 600, Loss: 1.139\n",
      "Epoch 7, Batch 700, Loss: 1.142\n",
      "Epoch 7, Batch 800, Loss: 1.162\n",
      "Epoch 7, Batch 900, Loss: 1.126\n",
      "Epoch 7, Batch 1000, Loss: 1.207\n",
      "Epoch 7, Batch 1100, Loss: 1.170\n",
      "Epoch 7, Batch 1200, Loss: 1.157\n",
      "Epoch 7, Batch 1300, Loss: 1.158\n",
      "Epoch 7, Batch 1400, Loss: 1.166\n",
      "Epoch 7, Batch 1500, Loss: 1.143\n",
      "Epoch 7, Batch 1600, Loss: 1.148\n",
      "Epoch 7, Batch 1700, Loss: 1.151\n",
      "Epoch 7, Batch 1800, Loss: 1.171\n",
      "Epoch 7, Batch 1900, Loss: 1.186\n",
      "Epoch 7, Batch 2000, Loss: 1.206\n",
      "Epoch 7, Batch 2100, Loss: 1.188\n",
      "Epoch 7, Batch 2200, Loss: 1.181\n",
      "Epoch 7, Batch 2300, Loss: 1.197\n",
      "Epoch 7, Batch 2400, Loss: 1.171\n",
      "Epoch 7, Batch 2500, Loss: 1.166\n",
      "Epoch 7, Batch 2600, Loss: 1.197\n",
      "Epoch 7, Batch 2700, Loss: 1.208\n",
      "Epoch 7, Batch 2800, Loss: 1.205\n",
      "Epoch 7, Batch 2900, Loss: 1.181\n",
      "Epoch 7, Batch 3000, Loss: 1.182\n",
      "Epoch 7, Batch 3100, Loss: 1.152\n",
      "Epoch 7, Batch 3200, Loss: 1.206\n",
      "Epoch 7, Batch 3300, Loss: 1.178\n",
      "Epoch 7, Batch 3400, Loss: 1.185\n",
      "Epoch 7, Batch 3500, Loss: 1.221\n",
      "Epoch 7, Batch 3600, Loss: 1.198\n",
      "Epoch 7, Batch 3700, Loss: 1.175\n",
      "Epoch 7, Batch 3800, Loss: 1.159\n",
      "Epoch 7, Batch 3900, Loss: 1.203\n",
      "Epoch 7, Batch 4000, Loss: 1.209\n",
      "Epoch 7, Batch 4100, Loss: 1.203\n",
      "Epoch 7, Batch 4200, Loss: 1.208\n",
      "Epoch 8, Batch 100, Loss: 1.055\n",
      "Epoch 8, Batch 200, Loss: 1.034\n",
      "Epoch 8, Batch 300, Loss: 1.065\n",
      "Epoch 8, Batch 400, Loss: 1.110\n",
      "Epoch 8, Batch 500, Loss: 1.028\n",
      "Epoch 8, Batch 600, Loss: 1.080\n",
      "Epoch 8, Batch 700, Loss: 1.105\n",
      "Epoch 8, Batch 800, Loss: 1.150\n",
      "Epoch 8, Batch 900, Loss: 1.085\n",
      "Epoch 8, Batch 1000, Loss: 1.116\n",
      "Epoch 8, Batch 1100, Loss: 1.097\n",
      "Epoch 8, Batch 1200, Loss: 1.090\n",
      "Epoch 8, Batch 1300, Loss: 1.127\n",
      "Epoch 8, Batch 1400, Loss: 1.109\n",
      "Epoch 8, Batch 1500, Loss: 1.119\n",
      "Epoch 8, Batch 1600, Loss: 1.112\n",
      "Epoch 8, Batch 1700, Loss: 1.134\n",
      "Epoch 8, Batch 1800, Loss: 1.146\n",
      "Epoch 8, Batch 1900, Loss: 1.134\n",
      "Epoch 8, Batch 2000, Loss: 1.118\n",
      "Epoch 8, Batch 2100, Loss: 1.104\n",
      "Epoch 8, Batch 2200, Loss: 1.130\n",
      "Epoch 8, Batch 2300, Loss: 1.136\n",
      "Epoch 8, Batch 2400, Loss: 1.165\n",
      "Epoch 8, Batch 2500, Loss: 1.141\n",
      "Epoch 8, Batch 2600, Loss: 1.162\n",
      "Epoch 8, Batch 2700, Loss: 1.153\n",
      "Epoch 8, Batch 2800, Loss: 1.171\n",
      "Epoch 8, Batch 2900, Loss: 1.184\n",
      "Epoch 8, Batch 3000, Loss: 1.125\n",
      "Epoch 8, Batch 3100, Loss: 1.150\n",
      "Epoch 8, Batch 3200, Loss: 1.139\n",
      "Epoch 8, Batch 3300, Loss: 1.130\n",
      "Epoch 8, Batch 3400, Loss: 1.148\n",
      "Epoch 8, Batch 3500, Loss: 1.184\n",
      "Epoch 8, Batch 3600, Loss: 1.119\n",
      "Epoch 8, Batch 3700, Loss: 1.175\n",
      "Epoch 8, Batch 3800, Loss: 1.177\n",
      "Epoch 8, Batch 3900, Loss: 1.195\n",
      "Epoch 8, Batch 4000, Loss: 1.125\n",
      "Epoch 8, Batch 4100, Loss: 1.197\n",
      "Epoch 8, Batch 4200, Loss: 1.188\n",
      "Epoch 9, Batch 100, Loss: 1.062\n",
      "Epoch 9, Batch 200, Loss: 1.016\n",
      "Epoch 9, Batch 300, Loss: 1.028\n",
      "Epoch 9, Batch 400, Loss: 1.051\n",
      "Epoch 9, Batch 500, Loss: 1.044\n",
      "Epoch 9, Batch 600, Loss: 1.048\n",
      "Epoch 9, Batch 700, Loss: 1.045\n",
      "Epoch 9, Batch 800, Loss: 1.070\n",
      "Epoch 9, Batch 900, Loss: 1.100\n",
      "Epoch 9, Batch 1000, Loss: 1.091\n",
      "Epoch 9, Batch 1100, Loss: 1.088\n",
      "Epoch 9, Batch 1200, Loss: 1.065\n",
      "Epoch 9, Batch 1300, Loss: 1.102\n",
      "Epoch 9, Batch 1400, Loss: 1.117\n",
      "Epoch 9, Batch 1500, Loss: 1.089\n",
      "Epoch 9, Batch 1600, Loss: 1.048\n",
      "Epoch 9, Batch 1700, Loss: 1.115\n",
      "Epoch 9, Batch 1800, Loss: 1.084\n",
      "Epoch 9, Batch 1900, Loss: 1.130\n",
      "Epoch 9, Batch 2000, Loss: 1.121\n",
      "Epoch 9, Batch 2100, Loss: 1.120\n",
      "Epoch 9, Batch 2200, Loss: 1.125\n",
      "Epoch 9, Batch 2300, Loss: 1.144\n",
      "Epoch 9, Batch 2400, Loss: 1.126\n",
      "Epoch 9, Batch 2500, Loss: 1.093\n",
      "Epoch 9, Batch 2600, Loss: 1.109\n",
      "Epoch 9, Batch 2700, Loss: 1.111\n",
      "Epoch 9, Batch 2800, Loss: 1.102\n",
      "Epoch 9, Batch 2900, Loss: 1.097\n",
      "Epoch 9, Batch 3000, Loss: 1.161\n",
      "Epoch 9, Batch 3100, Loss: 1.160\n",
      "Epoch 9, Batch 3200, Loss: 1.092\n",
      "Epoch 9, Batch 3300, Loss: 1.152\n",
      "Epoch 9, Batch 3400, Loss: 1.128\n",
      "Epoch 9, Batch 3500, Loss: 1.096\n",
      "Epoch 9, Batch 3600, Loss: 1.114\n",
      "Epoch 9, Batch 3700, Loss: 1.099\n",
      "Epoch 9, Batch 3800, Loss: 1.102\n",
      "Epoch 9, Batch 3900, Loss: 1.132\n",
      "Epoch 9, Batch 4000, Loss: 1.109\n",
      "Epoch 9, Batch 4100, Loss: 1.114\n",
      "Epoch 9, Batch 4200, Loss: 1.156\n",
      "Epoch 10, Batch 100, Loss: 1.031\n",
      "Epoch 10, Batch 200, Loss: 1.019\n",
      "Epoch 10, Batch 300, Loss: 1.035\n",
      "Epoch 10, Batch 400, Loss: 1.024\n",
      "Epoch 10, Batch 500, Loss: 1.072\n",
      "Epoch 10, Batch 600, Loss: 1.025\n",
      "Epoch 10, Batch 700, Loss: 1.054\n",
      "Epoch 10, Batch 800, Loss: 1.018\n",
      "Epoch 10, Batch 900, Loss: 1.055\n",
      "Epoch 10, Batch 1000, Loss: 1.036\n",
      "Epoch 10, Batch 1100, Loss: 1.047\n",
      "Epoch 10, Batch 1200, Loss: 1.017\n",
      "Epoch 10, Batch 1300, Loss: 1.078\n",
      "Epoch 10, Batch 1400, Loss: 1.082\n",
      "Epoch 10, Batch 1500, Loss: 1.073\n",
      "Epoch 10, Batch 1600, Loss: 1.057\n",
      "Epoch 10, Batch 1700, Loss: 1.062\n",
      "Epoch 10, Batch 1800, Loss: 1.084\n",
      "Epoch 10, Batch 1900, Loss: 1.111\n",
      "Epoch 10, Batch 2000, Loss: 1.108\n",
      "Epoch 10, Batch 2100, Loss: 1.074\n",
      "Epoch 10, Batch 2200, Loss: 1.080\n",
      "Epoch 10, Batch 2300, Loss: 1.081\n",
      "Epoch 10, Batch 2400, Loss: 1.093\n",
      "Epoch 10, Batch 2500, Loss: 1.085\n",
      "Epoch 10, Batch 2600, Loss: 1.097\n",
      "Epoch 10, Batch 2700, Loss: 1.075\n",
      "Epoch 10, Batch 2800, Loss: 1.087\n",
      "Epoch 10, Batch 2900, Loss: 1.082\n",
      "Epoch 10, Batch 3000, Loss: 1.097\n",
      "Epoch 10, Batch 3100, Loss: 1.115\n",
      "Epoch 10, Batch 3200, Loss: 1.071\n",
      "Epoch 10, Batch 3300, Loss: 1.115\n",
      "Epoch 10, Batch 3400, Loss: 1.112\n",
      "Epoch 10, Batch 3500, Loss: 1.070\n",
      "Epoch 10, Batch 3600, Loss: 1.087\n",
      "Epoch 10, Batch 3700, Loss: 1.095\n",
      "Epoch 10, Batch 3800, Loss: 1.135\n",
      "Epoch 10, Batch 3900, Loss: 1.143\n",
      "Epoch 10, Batch 4000, Loss: 1.105\n",
      "Epoch 10, Batch 4100, Loss: 1.106\n",
      "Epoch 10, Batch 4200, Loss: 1.130\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "from time import strftime, localtime\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from ai.deeplearning import ChessModel\n",
    "\n",
    "# 设置CuDNN选项\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ChessModel().to(device) # 初始化模型，并将其移动到GPU上\n",
    "criterion = nn.CrossEntropyLoss() # 定义交叉熵损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # 定义Adam优化器\n",
    "\n",
    "# 训练循环\n",
    "num_epochs = 10  # 训练10个epoch\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        inputs, labels = data\n",
    "        # 调整输入的维度，并将其移动到GPU上\n",
    "        # inputs 的原始形状是 (batch_size, height, width, channels)，也就是 (32, 5, 5, 3)\n",
    "        # inputs.permute(0, 3, 1, 2) 会将 inputs 的维度从 (32, 5, 5, 3) 转换为 (32, 3, 5, 5)\n",
    "        inputs = inputs.permute(0, 3, 1, 2).to(device)  # (batch_size, channels, height, width)\n",
    "        labels = labels.to(device).to(torch.int64)\n",
    "\n",
    "        # 清零梯度\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(inputs)\n",
    "        # 计算损失\n",
    "        loss = criterion(outputs, labels)\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        # 更新参数\n",
    "        optimizer.step()\n",
    "\n",
    "        # 累积损失\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # 每100个batch打印一次loss\n",
    "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "data_size = len(dataset)\n",
    "now_time = strftime(\"%m-%d-%H-%M\", localtime())\n",
    "add_text = '32-64-128-256'\n",
    "torch.save(model.state_dict(), f'models/dl_model({now_time})({data_size})({add_text}).pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 确定项目根目录（假设当前工作目录是项目的根目录）\n",
    "project_root = os.path.abspath(\"..\")\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "from ai import MinimaxAI, MCTSAI\n",
    "from ai.test_ai import get_model_score_by_mcts\n",
    "from ai.deeplearning import DeepLearningAI\n",
    "from game.chess_game import ChessGame\n",
    "\n",
    "\n",
    "model_score_dict = {\n",
    "    \"MCTSAI(1000, complate_mode=False)\": (990, ),\n",
    "    \"MCTSAI(100, complate_mode=False)\": (110, {10: 0.79, 20: 0.71, 30: 0.68, 40: 0.595, 50: 0.565, 60: 0.585, 70: 0.615, 80: 0.54, 90: 0.555, 100: 0.435, 110: 0.53, 120: 0.415}),\n",
    "    \"MCTSAI(50, complate_mode=False)\": (40, {10: 0.72, 20: 0.605, 30: 0.565, 40: 0.535, 50: 0.535}),\n",
    "    \"MinimaxAI(5, *game_state, complate_mode=False)\": (0, {10: 0.495}),\n",
    "    \"MinimaxAI(5, *game_state, complate_mode=True)\": (0, {10: 0.47}),\n",
    "    \"MinimaxAI(3, *game_state, complate_mode=False)\": (0, {10: 0.44}),\n",
    "    'DeepLearningAI(\"models/dl_model(06-22-16)(127)(16-32-64).pth\")': (0, {10: 0.36}),\n",
    "    'DeepLearningAI(\"models/dl_model(06-22-20)(136090)(16-32-64).pth\")': (60, {10: 0.74, 20: 0.575, 30: 0.675, 40: 0.615, 50: 0.545, 60: 0.56, 70: 0.46}),\n",
    "    'DeepLearningAI(\"models/dl_model(06-22-20)(136090)(32-64-128).pth\")': (120, {10: 0.77, 20: 0.8, 30: 0.715, 40: 0.615, 50: 0.625, 60: 0.655, 70: 0.505, 80: 0.575, 90: 0.53, 100: 0.575, 110: 0.53, 120: 0.46, 130: 0.44}),\n",
    "    'DeepLearningAI(\"models/dl_model(06-22-21-09)(136090)(32-64-64-128).pth\", complate_mode=False)': (30, {10: 0.69, 20: 0.63, 30: 0.55, 40: 0.5}),\n",
    "    'DeepLearningAI(\"models/dl_model(06-22-21-18)(136090)(32-64-128-256).pth\", complate_mode=False)': (190, {10: 0.78, 20: 0.69, 30: 0.73, 40: 0.775, 50: 0.675, 60: 0.635, 70: 0.505, 80: 0.595, 90: 0.505, 100: 0.575, 110: 0.545, 120: 0.63, 130: 0.485, 140: 0.505, 150: 0.58, 160: 0.62, 170: 0.48, 180: 0.595, 190: 0.5, 200: 0.57}),\n",
    "    \n",
    "\n",
    "}\n",
    "\n",
    "# 与其他AI算法进行对战\n",
    "# ai_battle(deep_learning_ai_2, test_mcts_0, ChessGame((5, 5), 2), display=True)\n",
    "game_state = ((5,5), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_model_score_by_mcts() missing 1 required positional argument: 'game_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mget_model_score_by_mcts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMCTSAI\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomplate_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: get_model_score_by_mcts() missing 1 required positional argument: 'game_state'"
     ]
    }
   ],
   "source": [
    "print(get_model_score_by_mcts(MCTSAI(50, complate_mode=False), game_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "__enter__",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m MCTSAI(\u001b[38;5;241m100\u001b[39m, complate_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m test_ai:\n\u001b[0;32m      2\u001b[0m     score,score_dict \u001b[38;5;241m=\u001b[39m get_model_score_by_mcts(test_ai)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(score,score_dict)\n",
      "\u001b[1;31mAttributeError\u001b[0m: __enter__"
     ]
    }
   ],
   "source": [
    "with MCTSAI(100, complate_mode=False) as test_ai:\n",
    "    score,score_dict = get_model_score_by_mcts(test_ai)\n",
    "    print(score,score_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 19/999 [01:12<1:02:35,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(190, {10: 0.78, 20: 0.69, 30: 0.73, 40: 0.775, 50: 0.675, 60: 0.635, 70: 0.505, 80: 0.595, 90: 0.505, 100: 0.575, 110: 0.545, 120: 0.63, 130: 0.485, 140: 0.505, 150: 0.58, 160: 0.62, 170: 0.48, 180: 0.595, 190: 0.5, 200: 0.57})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(get_model_score_by_mcts(DeepLearningAI(\"models/dl_model(06-22-21-18)(136090)(32-64-128-256).pth\", complate_mode=False), game_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deeplearning_model = DeepLearningAI(r\"models\\chess_ai_model(06-15-17)(15475).pth\") \n",
    "\n",
    "len(deeplearning_model.model.conv1.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ChessModel:\n\tMissing key(s) in state_dict: \"conv3.weight\", \"conv3.bias\". \n\tsize mismatch for conv1.weight: copying a param with shape torch.Size([32, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 3, 3, 3]).\n\tsize mismatch for conv1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for conv2.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 16, 3, 3]).\n\tsize mismatch for conv2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m deep_learning_dict\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m----> 2\u001b[0m     deep_learning_ai \u001b[38;5;241m=\u001b[39m \u001b[43mDeepLearningAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     score, score_dict \u001b[38;5;241m=\u001b[39m get_model_score_by_mcts(deep_learning_ai)\n\u001b[0;32m      4\u001b[0m     deep_learning_dict[path] \u001b[38;5;241m=\u001b[39m score\n",
      "File \u001b[1;32mq:\\Project\\Celestial-Chess\\ai\\deeplearning.py:41\u001b[0m, in \u001b[0;36mDeepLearningAI.__init__\u001b[1;34m(self, model_path)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m ChessModel()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mq:\\ProgramData\\miniforge3\\envs\\toolenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ChessModel:\n\tMissing key(s) in state_dict: \"conv3.weight\", \"conv3.bias\". \n\tsize mismatch for conv1.weight: copying a param with shape torch.Size([32, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 3, 3, 3]).\n\tsize mismatch for conv1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for conv2.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 16, 3, 3]).\n\tsize mismatch for conv2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32])."
     ]
    }
   ],
   "source": [
    "for path in deep_learning_dict.keys():\n",
    "    deep_learning_ai = DeepLearningAI(path)\n",
    "    score, score_dict = get_model_score_by_mcts(deep_learning_ai)\n",
    "    deep_learning_dict[path] = score\n",
    "    print(f\"{path} : {score_dict}\", end = '\\n')\n",
    "\n",
    "deep_learning_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 深度学习AI\n",
    "deep_learning_dict = {\n",
    "                      # {10: 0.7, 20: 0.68, 30: 0.47}\n",
    "                      # \n",
    "                      \"models/chess_ai_model(06-13-19).pth\": 20, # 未知\n",
    "                      # {10: 0.7, 20: 0.51, 30: 0.43}\n",
    "                      # \n",
    "                      \"models/chess_ai_model(06-15-17)(1506).pth\": 20, # MCTSAI(1000, flag=True) 训练100轮\n",
    "                      # {10: 0.75, 20: 0.65, 30: 0.47}\n",
    "                      # \n",
    "                      'models/chess_ai_model(06-15-17)(1471).pth': 20, # MCTSAI(10000, flag=True) 训练100轮\n",
    "                      # {10: 0.63, 20: 0.41}\n",
    "                      'models/chess_ai_model(06-15-17)(936).pth': 10, # MCTSAI(10000, flag=True) 训练100轮并去重\n",
    "                      # {10: 0.82, 20: 0.76, 30: 0.65, 40: 0.57, 50: 0.56, 60: 0.44}\n",
    "                      # \n",
    "                      'models/chess_ai_model(06-15-17)(15475).pth': 50, # MCTSAI(1000, flag=True) 训练1000轮\n",
    "                      # {10: 0.84, 20: 0.58, 30: 0.57, 40: 0.61, 50: 0.56, 60: 0.41}\n",
    "                      'models/chess_ai_model(06-15-17)(9598).pth': 50, # MCTSAI(1000, flag=True) 训练1000轮并去重\n",
    "                      # {10: 0.32}\n",
    "                      \"models/chess_ai_model(06-16-15)(1400).pth\": 0, # MCTSAI(10000, flag=False) 训练100轮(没标错, 真是0分)\n",
    "                      # {10: 0.55, 20: 0.48}\n",
    "                      'models/chess_ai_model(06-16-20)(1542).pth': 10, # MCTSAI(1000, flag=False) 训练100轮\n",
    "\n",
    "                      # {10: 0.82, 20: 0.73, 30: 0.67, 40: 0.52, 50: 0.46}\n",
    "                      # \n",
    "                      'models/chess_ai_model(06-17-12)(15475).pth': 40, # MCTSAI(1000, flag=True) 训练1000轮, 使用三层32-64-128卷积层, 128-256-25全连接层\n",
    "                      # {10: 0.72, 20: 0.39}\n",
    "                      'models/chess_ai_model(06-17-12)(1506).pth': 10, # MCTSAI(1000, flag=True) 训练100轮, 使用三层32-64-128卷积层, 128-256-25全连接层\n",
    "                      \n",
    "                      # {10: 0.9, 20: 0.84, 30: 0.72, 40: 0.6, 50: 0.48}\n",
    "                      # {10: 0.88, 20: 0.83, 30: 0.69, 40: 0.68, 50: 0.57, 60: 0.65, 70: 0.61, 80: 0.63, 90: 0.49}\n",
    "                      # \n",
    "                      'models/chess_ai_model(06-17-13)(15475).pth': 80, # MCTSAI(1000, flag=True) 训练1000轮, 使用三层16-32-64卷积层, 64-128-25全连接层\n",
    "                      }\n",
    "\n",
    "                    \n",
    "# deep_learning_ai_0 = DeepLearningAI(\"models/chess_ai_model(06-15-17)(936).pth\") \n",
    "\n",
    "# 测试AI\n",
    "test_minimax = MinimaxAI(6) # score: 50\n",
    "test_mcts_0 = MCTSAI(1000) # \n",
    "test_mcts_1 = MCTSAI(100, flag=True) # score: 50\n",
    "test_mcts_2 = MCTSAI(100, flag=False) # score: 80\n",
    "test_mcts_3 = MCTSAI(60, flag=True) # 50, {10: 0.82, 20: 0.76, 30: 0.53, 40: 0.57, 50: 0.51, 60: 0.32}\n",
    "test_mcts_4 = MCTSAI(60, flag=False) # 110, {10: 0.86, 20: 0.86, 30: 0.77, 40: 0.63, 50: 0.58, 60: 0.63, 70: 0.58, 80: 0.52, 90: 0.51, 100: 0.5, 110: 0.52, 120: 0.45}\n",
    "test_mcts_5 = MCTSAI(10, flag=True) # 0, {10: 0.45}\n",
    "test_mcts_6 = MCTSAI(10, flag=False) # 50, {10: 0.76, 20: 0.69, 30: 0.57, 40: 0.56, 50: 0.5, 60: 0.4}\n",
    "test_mcts_7 = MCTSAI(500, flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chessenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
