{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "flag = torch.cuda.is_available()\n",
    "print(flag)\n",
    "\n",
    "if flag:\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(torch.rand(3,3).cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "trainDataProcess(serial):   0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "trainDataProcess(serial): 100%|██████████| 11/11 [00:07<00:00,  1.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[[ 0.,  0.,  1.],\n",
       "         [-1.,  1.,  1.],\n",
       "         [-2.,  2.,  1.],\n",
       "         [-1.,  1.,  1.],\n",
       "         [ 0.,  0.,  1.]],\n",
       " \n",
       "        [[ 0.,  0.,  1.],\n",
       "         [ 0.,  0.,  1.],\n",
       "         [-1.,  1.,  1.],\n",
       "         [ 0.,  0.,  1.],\n",
       "         [ 0.,  0.,  1.]],\n",
       " \n",
       "        [[ 1.,  1.,  1.],\n",
       "         [ 1.,  1.,  1.],\n",
       "         [ 2.,  2.,  1.],\n",
       "         [ 2.,  2.,  1.],\n",
       "         [ 2.,  2.,  1.]],\n",
       " \n",
       "        [[ 0.,  0.,  1.],\n",
       "         [ 0.,  0.,  1.],\n",
       "         [ 0.,  2.,  1.],\n",
       "         [-2.,  2.,  1.],\n",
       "         [ 0.,  2.,  1.]],\n",
       " \n",
       "        [[ 0.,  0.,  1.],\n",
       "         [ 0.,  0.,  1.],\n",
       "         [ 0.,  0.,  1.],\n",
       "         [-1.,  1.,  1.],\n",
       "         [ 0.,  0.,  1.]]]),\n",
       " array([3, 0]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from train_ai import start_train_data, load_train_data\n",
    "\n",
    "train_data = start_train_data(10, 1000, 'serial')\n",
    "\n",
    "# train_data_path = r\"train_data\\2024-06-22\\MCTS1k_Games10k_Length136090(17).pkl\"\n",
    "# train_data = load_train_data(train_data_path)\n",
    "\n",
    "train_data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100, Loss: 3.153\n",
      "Epoch 1, Batch 200, Loss: 3.066\n",
      "Epoch 1, Batch 300, Loss: 2.957\n",
      "Epoch 1, Batch 400, Loss: 2.880\n",
      "Epoch 1, Batch 500, Loss: 2.781\n",
      "Epoch 1, Batch 600, Loss: 2.719\n",
      "Epoch 1, Batch 700, Loss: 2.656\n",
      "Epoch 1, Batch 800, Loss: 2.616\n",
      "Epoch 1, Batch 900, Loss: 2.557\n",
      "Epoch 1, Batch 1000, Loss: 2.526\n",
      "Epoch 1, Batch 1100, Loss: 2.462\n",
      "Epoch 1, Batch 1200, Loss: 2.420\n",
      "Epoch 1, Batch 1300, Loss: 2.366\n",
      "Epoch 1, Batch 1400, Loss: 2.306\n",
      "Epoch 1, Batch 1500, Loss: 2.287\n",
      "Epoch 1, Batch 1600, Loss: 2.242\n",
      "Epoch 1, Batch 1700, Loss: 2.197\n",
      "Epoch 1, Batch 1800, Loss: 2.149\n",
      "Epoch 1, Batch 1900, Loss: 2.130\n",
      "Epoch 1, Batch 2000, Loss: 2.068\n",
      "Epoch 1, Batch 2100, Loss: 2.027\n",
      "Epoch 1, Batch 2200, Loss: 2.003\n",
      "Epoch 1, Batch 2300, Loss: 1.981\n",
      "Epoch 1, Batch 2400, Loss: 1.916\n",
      "Epoch 1, Batch 2500, Loss: 1.905\n",
      "Epoch 1, Batch 2600, Loss: 1.846\n",
      "Epoch 1, Batch 2700, Loss: 1.801\n",
      "Epoch 1, Batch 2800, Loss: 1.807\n",
      "Epoch 1, Batch 2900, Loss: 1.774\n",
      "Epoch 1, Batch 3000, Loss: 1.752\n",
      "Epoch 1, Batch 3100, Loss: 1.752\n",
      "Epoch 1, Batch 3200, Loss: 1.734\n",
      "Epoch 1, Batch 3300, Loss: 1.696\n",
      "Epoch 1, Batch 3400, Loss: 1.710\n",
      "Epoch 1, Batch 3500, Loss: 1.678\n",
      "Epoch 1, Batch 3600, Loss: 1.629\n",
      "Epoch 1, Batch 3700, Loss: 1.676\n",
      "Epoch 1, Batch 3800, Loss: 1.644\n",
      "Epoch 1, Batch 3900, Loss: 1.625\n",
      "Epoch 1, Batch 4000, Loss: 1.659\n",
      "Epoch 1, Batch 4100, Loss: 1.621\n",
      "Epoch 1, Batch 4200, Loss: 1.628\n",
      "Epoch 2, Batch 100, Loss: 1.571\n",
      "Epoch 2, Batch 200, Loss: 1.581\n",
      "Epoch 2, Batch 300, Loss: 1.548\n",
      "Epoch 2, Batch 400, Loss: 1.553\n",
      "Epoch 2, Batch 500, Loss: 1.557\n",
      "Epoch 2, Batch 600, Loss: 1.562\n",
      "Epoch 2, Batch 700, Loss: 1.528\n",
      "Epoch 2, Batch 800, Loss: 1.571\n",
      "Epoch 2, Batch 900, Loss: 1.559\n",
      "Epoch 2, Batch 1000, Loss: 1.565\n",
      "Epoch 2, Batch 1100, Loss: 1.537\n",
      "Epoch 2, Batch 1200, Loss: 1.573\n",
      "Epoch 2, Batch 1300, Loss: 1.546\n",
      "Epoch 2, Batch 1400, Loss: 1.514\n",
      "Epoch 2, Batch 1500, Loss: 1.526\n",
      "Epoch 2, Batch 1600, Loss: 1.563\n",
      "Epoch 2, Batch 1700, Loss: 1.551\n",
      "Epoch 2, Batch 1800, Loss: 1.536\n",
      "Epoch 2, Batch 1900, Loss: 1.550\n",
      "Epoch 2, Batch 2000, Loss: 1.486\n",
      "Epoch 2, Batch 2100, Loss: 1.539\n",
      "Epoch 2, Batch 2200, Loss: 1.545\n",
      "Epoch 2, Batch 2300, Loss: 1.502\n",
      "Epoch 2, Batch 2400, Loss: 1.501\n",
      "Epoch 2, Batch 2500, Loss: 1.532\n",
      "Epoch 2, Batch 2600, Loss: 1.506\n",
      "Epoch 2, Batch 2700, Loss: 1.462\n",
      "Epoch 2, Batch 2800, Loss: 1.511\n",
      "Epoch 2, Batch 2900, Loss: 1.494\n",
      "Epoch 2, Batch 3000, Loss: 1.520\n",
      "Epoch 2, Batch 3100, Loss: 1.495\n",
      "Epoch 2, Batch 3200, Loss: 1.509\n",
      "Epoch 2, Batch 3300, Loss: 1.532\n",
      "Epoch 2, Batch 3400, Loss: 1.486\n",
      "Epoch 2, Batch 3500, Loss: 1.455\n",
      "Epoch 2, Batch 3600, Loss: 1.508\n",
      "Epoch 2, Batch 3700, Loss: 1.503\n",
      "Epoch 2, Batch 3800, Loss: 1.515\n",
      "Epoch 2, Batch 3900, Loss: 1.506\n",
      "Epoch 2, Batch 4000, Loss: 1.483\n",
      "Epoch 2, Batch 4100, Loss: 1.522\n",
      "Epoch 2, Batch 4200, Loss: 1.492\n",
      "Epoch 3, Batch 100, Loss: 1.434\n",
      "Epoch 3, Batch 200, Loss: 1.437\n",
      "Epoch 3, Batch 300, Loss: 1.401\n",
      "Epoch 3, Batch 400, Loss: 1.430\n",
      "Epoch 3, Batch 500, Loss: 1.415\n",
      "Epoch 3, Batch 600, Loss: 1.425\n",
      "Epoch 3, Batch 700, Loss: 1.423\n",
      "Epoch 3, Batch 800, Loss: 1.399\n",
      "Epoch 3, Batch 900, Loss: 1.369\n",
      "Epoch 3, Batch 1000, Loss: 1.392\n",
      "Epoch 3, Batch 1100, Loss: 1.442\n",
      "Epoch 3, Batch 1200, Loss: 1.391\n",
      "Epoch 3, Batch 1300, Loss: 1.415\n",
      "Epoch 3, Batch 1400, Loss: 1.406\n",
      "Epoch 3, Batch 1500, Loss: 1.419\n",
      "Epoch 3, Batch 1600, Loss: 1.412\n",
      "Epoch 3, Batch 1700, Loss: 1.460\n",
      "Epoch 3, Batch 1800, Loss: 1.376\n",
      "Epoch 3, Batch 1900, Loss: 1.396\n",
      "Epoch 3, Batch 2000, Loss: 1.381\n",
      "Epoch 3, Batch 2100, Loss: 1.427\n",
      "Epoch 3, Batch 2200, Loss: 1.428\n",
      "Epoch 3, Batch 2300, Loss: 1.446\n",
      "Epoch 3, Batch 2400, Loss: 1.416\n",
      "Epoch 3, Batch 2500, Loss: 1.399\n",
      "Epoch 3, Batch 2600, Loss: 1.401\n",
      "Epoch 3, Batch 2700, Loss: 1.402\n",
      "Epoch 3, Batch 2800, Loss: 1.422\n",
      "Epoch 3, Batch 2900, Loss: 1.427\n",
      "Epoch 3, Batch 3000, Loss: 1.419\n",
      "Epoch 3, Batch 3100, Loss: 1.372\n",
      "Epoch 3, Batch 3200, Loss: 1.402\n",
      "Epoch 3, Batch 3300, Loss: 1.421\n",
      "Epoch 3, Batch 3400, Loss: 1.364\n",
      "Epoch 3, Batch 3500, Loss: 1.383\n",
      "Epoch 3, Batch 3600, Loss: 1.404\n",
      "Epoch 3, Batch 3700, Loss: 1.484\n",
      "Epoch 3, Batch 3800, Loss: 1.397\n",
      "Epoch 3, Batch 3900, Loss: 1.422\n",
      "Epoch 3, Batch 4000, Loss: 1.415\n",
      "Epoch 3, Batch 4100, Loss: 1.422\n",
      "Epoch 3, Batch 4200, Loss: 1.391\n",
      "Epoch 4, Batch 100, Loss: 1.278\n",
      "Epoch 4, Batch 200, Loss: 1.311\n",
      "Epoch 4, Batch 300, Loss: 1.290\n",
      "Epoch 4, Batch 400, Loss: 1.294\n",
      "Epoch 4, Batch 500, Loss: 1.308\n",
      "Epoch 4, Batch 600, Loss: 1.296\n",
      "Epoch 4, Batch 700, Loss: 1.362\n",
      "Epoch 4, Batch 800, Loss: 1.305\n",
      "Epoch 4, Batch 900, Loss: 1.321\n",
      "Epoch 4, Batch 1000, Loss: 1.298\n",
      "Epoch 4, Batch 1100, Loss: 1.315\n",
      "Epoch 4, Batch 1200, Loss: 1.315\n",
      "Epoch 4, Batch 1300, Loss: 1.345\n",
      "Epoch 4, Batch 1400, Loss: 1.359\n",
      "Epoch 4, Batch 1500, Loss: 1.355\n",
      "Epoch 4, Batch 1600, Loss: 1.334\n",
      "Epoch 4, Batch 1700, Loss: 1.332\n",
      "Epoch 4, Batch 1800, Loss: 1.368\n",
      "Epoch 4, Batch 1900, Loss: 1.327\n",
      "Epoch 4, Batch 2000, Loss: 1.307\n",
      "Epoch 4, Batch 2100, Loss: 1.362\n",
      "Epoch 4, Batch 2200, Loss: 1.311\n",
      "Epoch 4, Batch 2300, Loss: 1.314\n",
      "Epoch 4, Batch 2400, Loss: 1.364\n",
      "Epoch 4, Batch 2500, Loss: 1.319\n",
      "Epoch 4, Batch 2600, Loss: 1.353\n",
      "Epoch 4, Batch 2700, Loss: 1.339\n",
      "Epoch 4, Batch 2800, Loss: 1.348\n",
      "Epoch 4, Batch 2900, Loss: 1.338\n",
      "Epoch 4, Batch 3000, Loss: 1.319\n",
      "Epoch 4, Batch 3100, Loss: 1.354\n",
      "Epoch 4, Batch 3200, Loss: 1.357\n",
      "Epoch 4, Batch 3300, Loss: 1.345\n",
      "Epoch 4, Batch 3400, Loss: 1.347\n",
      "Epoch 4, Batch 3500, Loss: 1.313\n",
      "Epoch 4, Batch 3600, Loss: 1.331\n",
      "Epoch 4, Batch 3700, Loss: 1.316\n",
      "Epoch 4, Batch 3800, Loss: 1.337\n",
      "Epoch 4, Batch 3900, Loss: 1.342\n",
      "Epoch 4, Batch 4000, Loss: 1.324\n",
      "Epoch 4, Batch 4100, Loss: 1.342\n",
      "Epoch 4, Batch 4200, Loss: 1.359\n",
      "Epoch 5, Batch 100, Loss: 1.231\n",
      "Epoch 5, Batch 200, Loss: 1.226\n",
      "Epoch 5, Batch 300, Loss: 1.206\n",
      "Epoch 5, Batch 400, Loss: 1.238\n",
      "Epoch 5, Batch 500, Loss: 1.250\n",
      "Epoch 5, Batch 600, Loss: 1.207\n",
      "Epoch 5, Batch 700, Loss: 1.241\n",
      "Epoch 5, Batch 800, Loss: 1.271\n",
      "Epoch 5, Batch 900, Loss: 1.257\n",
      "Epoch 5, Batch 1000, Loss: 1.244\n",
      "Epoch 5, Batch 1100, Loss: 1.248\n",
      "Epoch 5, Batch 1200, Loss: 1.264\n",
      "Epoch 5, Batch 1300, Loss: 1.248\n",
      "Epoch 5, Batch 1400, Loss: 1.263\n",
      "Epoch 5, Batch 1500, Loss: 1.224\n",
      "Epoch 5, Batch 1600, Loss: 1.249\n",
      "Epoch 5, Batch 1700, Loss: 1.244\n",
      "Epoch 5, Batch 1800, Loss: 1.263\n",
      "Epoch 5, Batch 1900, Loss: 1.289\n",
      "Epoch 5, Batch 2000, Loss: 1.257\n",
      "Epoch 5, Batch 2100, Loss: 1.272\n",
      "Epoch 5, Batch 2200, Loss: 1.295\n",
      "Epoch 5, Batch 2300, Loss: 1.283\n",
      "Epoch 5, Batch 2400, Loss: 1.251\n",
      "Epoch 5, Batch 2500, Loss: 1.294\n",
      "Epoch 5, Batch 2600, Loss: 1.269\n",
      "Epoch 5, Batch 2700, Loss: 1.260\n",
      "Epoch 5, Batch 2800, Loss: 1.276\n",
      "Epoch 5, Batch 2900, Loss: 1.283\n",
      "Epoch 5, Batch 3000, Loss: 1.294\n",
      "Epoch 5, Batch 3100, Loss: 1.276\n",
      "Epoch 5, Batch 3200, Loss: 1.274\n",
      "Epoch 5, Batch 3300, Loss: 1.233\n",
      "Epoch 5, Batch 3400, Loss: 1.286\n",
      "Epoch 5, Batch 3500, Loss: 1.264\n",
      "Epoch 5, Batch 3600, Loss: 1.267\n",
      "Epoch 5, Batch 3700, Loss: 1.251\n",
      "Epoch 5, Batch 3800, Loss: 1.313\n",
      "Epoch 5, Batch 3900, Loss: 1.269\n",
      "Epoch 5, Batch 4000, Loss: 1.290\n",
      "Epoch 5, Batch 4100, Loss: 1.257\n",
      "Epoch 5, Batch 4200, Loss: 1.298\n",
      "Epoch 6, Batch 100, Loss: 1.148\n",
      "Epoch 6, Batch 200, Loss: 1.137\n",
      "Epoch 6, Batch 300, Loss: 1.171\n",
      "Epoch 6, Batch 400, Loss: 1.143\n",
      "Epoch 6, Batch 500, Loss: 1.172\n",
      "Epoch 6, Batch 600, Loss: 1.186\n",
      "Epoch 6, Batch 700, Loss: 1.157\n",
      "Epoch 6, Batch 800, Loss: 1.209\n",
      "Epoch 6, Batch 900, Loss: 1.197\n",
      "Epoch 6, Batch 1000, Loss: 1.147\n",
      "Epoch 6, Batch 1100, Loss: 1.182\n",
      "Epoch 6, Batch 1200, Loss: 1.204\n",
      "Epoch 6, Batch 1300, Loss: 1.215\n",
      "Epoch 6, Batch 1400, Loss: 1.197\n",
      "Epoch 6, Batch 1500, Loss: 1.194\n",
      "Epoch 6, Batch 1600, Loss: 1.181\n",
      "Epoch 6, Batch 1700, Loss: 1.204\n",
      "Epoch 6, Batch 1800, Loss: 1.227\n",
      "Epoch 6, Batch 1900, Loss: 1.209\n",
      "Epoch 6, Batch 2000, Loss: 1.209\n",
      "Epoch 6, Batch 2100, Loss: 1.204\n",
      "Epoch 6, Batch 2200, Loss: 1.205\n",
      "Epoch 6, Batch 2300, Loss: 1.225\n",
      "Epoch 6, Batch 2400, Loss: 1.198\n",
      "Epoch 6, Batch 2500, Loss: 1.183\n",
      "Epoch 6, Batch 2600, Loss: 1.182\n",
      "Epoch 6, Batch 2700, Loss: 1.179\n",
      "Epoch 6, Batch 2800, Loss: 1.216\n",
      "Epoch 6, Batch 2900, Loss: 1.236\n",
      "Epoch 6, Batch 3000, Loss: 1.248\n",
      "Epoch 6, Batch 3100, Loss: 1.254\n",
      "Epoch 6, Batch 3200, Loss: 1.229\n",
      "Epoch 6, Batch 3300, Loss: 1.251\n",
      "Epoch 6, Batch 3400, Loss: 1.202\n",
      "Epoch 6, Batch 3500, Loss: 1.237\n",
      "Epoch 6, Batch 3600, Loss: 1.208\n",
      "Epoch 6, Batch 3700, Loss: 1.226\n",
      "Epoch 6, Batch 3800, Loss: 1.238\n",
      "Epoch 6, Batch 3900, Loss: 1.252\n",
      "Epoch 6, Batch 4000, Loss: 1.236\n",
      "Epoch 6, Batch 4100, Loss: 1.281\n",
      "Epoch 6, Batch 4200, Loss: 1.230\n",
      "Epoch 7, Batch 100, Loss: 1.100\n",
      "Epoch 7, Batch 200, Loss: 1.077\n",
      "Epoch 7, Batch 300, Loss: 1.084\n",
      "Epoch 7, Batch 400, Loss: 1.098\n",
      "Epoch 7, Batch 500, Loss: 1.094\n",
      "Epoch 7, Batch 600, Loss: 1.116\n",
      "Epoch 7, Batch 700, Loss: 1.104\n",
      "Epoch 7, Batch 800, Loss: 1.124\n",
      "Epoch 7, Batch 900, Loss: 1.092\n",
      "Epoch 7, Batch 1000, Loss: 1.122\n",
      "Epoch 7, Batch 1100, Loss: 1.153\n",
      "Epoch 7, Batch 1200, Loss: 1.167\n",
      "Epoch 7, Batch 1300, Loss: 1.117\n",
      "Epoch 7, Batch 1400, Loss: 1.151\n",
      "Epoch 7, Batch 1500, Loss: 1.149\n",
      "Epoch 7, Batch 1600, Loss: 1.181\n",
      "Epoch 7, Batch 1700, Loss: 1.153\n",
      "Epoch 7, Batch 1800, Loss: 1.145\n",
      "Epoch 7, Batch 1900, Loss: 1.148\n",
      "Epoch 7, Batch 2000, Loss: 1.147\n",
      "Epoch 7, Batch 2100, Loss: 1.208\n",
      "Epoch 7, Batch 2200, Loss: 1.197\n",
      "Epoch 7, Batch 2300, Loss: 1.150\n",
      "Epoch 7, Batch 2400, Loss: 1.139\n",
      "Epoch 7, Batch 2500, Loss: 1.164\n",
      "Epoch 7, Batch 2600, Loss: 1.165\n",
      "Epoch 7, Batch 2700, Loss: 1.168\n",
      "Epoch 7, Batch 2800, Loss: 1.187\n",
      "Epoch 7, Batch 2900, Loss: 1.164\n",
      "Epoch 7, Batch 3000, Loss: 1.222\n",
      "Epoch 7, Batch 3100, Loss: 1.199\n",
      "Epoch 7, Batch 3200, Loss: 1.202\n",
      "Epoch 7, Batch 3300, Loss: 1.174\n",
      "Epoch 7, Batch 3400, Loss: 1.193\n",
      "Epoch 7, Batch 3500, Loss: 1.181\n",
      "Epoch 7, Batch 3600, Loss: 1.201\n",
      "Epoch 7, Batch 3700, Loss: 1.173\n",
      "Epoch 7, Batch 3800, Loss: 1.194\n",
      "Epoch 7, Batch 3900, Loss: 1.185\n",
      "Epoch 7, Batch 4000, Loss: 1.190\n",
      "Epoch 7, Batch 4100, Loss: 1.186\n",
      "Epoch 7, Batch 4200, Loss: 1.209\n",
      "Epoch 8, Batch 100, Loss: 1.027\n",
      "Epoch 8, Batch 200, Loss: 1.044\n",
      "Epoch 8, Batch 300, Loss: 1.059\n",
      "Epoch 8, Batch 400, Loss: 1.104\n",
      "Epoch 8, Batch 500, Loss: 1.102\n",
      "Epoch 8, Batch 600, Loss: 1.077\n",
      "Epoch 8, Batch 700, Loss: 1.047\n",
      "Epoch 8, Batch 800, Loss: 1.053\n",
      "Epoch 8, Batch 900, Loss: 1.103\n",
      "Epoch 8, Batch 1000, Loss: 1.088\n",
      "Epoch 8, Batch 1100, Loss: 1.071\n",
      "Epoch 8, Batch 1200, Loss: 1.077\n",
      "Epoch 8, Batch 1300, Loss: 1.091\n",
      "Epoch 8, Batch 1400, Loss: 1.093\n",
      "Epoch 8, Batch 1500, Loss: 1.103\n",
      "Epoch 8, Batch 1600, Loss: 1.113\n",
      "Epoch 8, Batch 1700, Loss: 1.124\n",
      "Epoch 8, Batch 1800, Loss: 1.116\n",
      "Epoch 8, Batch 1900, Loss: 1.164\n",
      "Epoch 8, Batch 2000, Loss: 1.109\n",
      "Epoch 8, Batch 2100, Loss: 1.105\n",
      "Epoch 8, Batch 2200, Loss: 1.126\n",
      "Epoch 8, Batch 2300, Loss: 1.123\n",
      "Epoch 8, Batch 2400, Loss: 1.145\n",
      "Epoch 8, Batch 2500, Loss: 1.141\n",
      "Epoch 8, Batch 2600, Loss: 1.115\n",
      "Epoch 8, Batch 2700, Loss: 1.112\n",
      "Epoch 8, Batch 2800, Loss: 1.159\n",
      "Epoch 8, Batch 2900, Loss: 1.132\n",
      "Epoch 8, Batch 3000, Loss: 1.154\n",
      "Epoch 8, Batch 3100, Loss: 1.138\n",
      "Epoch 8, Batch 3200, Loss: 1.113\n",
      "Epoch 8, Batch 3300, Loss: 1.120\n",
      "Epoch 8, Batch 3400, Loss: 1.117\n",
      "Epoch 8, Batch 3500, Loss: 1.150\n",
      "Epoch 8, Batch 3600, Loss: 1.129\n",
      "Epoch 8, Batch 3700, Loss: 1.144\n",
      "Epoch 8, Batch 3800, Loss: 1.175\n",
      "Epoch 8, Batch 3900, Loss: 1.186\n",
      "Epoch 8, Batch 4000, Loss: 1.188\n",
      "Epoch 8, Batch 4100, Loss: 1.171\n",
      "Epoch 8, Batch 4200, Loss: 1.174\n",
      "Epoch 9, Batch 100, Loss: 1.043\n",
      "Epoch 9, Batch 200, Loss: 0.978\n",
      "Epoch 9, Batch 300, Loss: 1.013\n",
      "Epoch 9, Batch 400, Loss: 1.017\n",
      "Epoch 9, Batch 500, Loss: 1.028\n",
      "Epoch 9, Batch 600, Loss: 1.070\n",
      "Epoch 9, Batch 700, Loss: 1.037\n",
      "Epoch 9, Batch 800, Loss: 1.053\n",
      "Epoch 9, Batch 900, Loss: 1.049\n",
      "Epoch 9, Batch 1000, Loss: 1.091\n",
      "Epoch 9, Batch 1100, Loss: 1.094\n",
      "Epoch 9, Batch 1200, Loss: 1.066\n",
      "Epoch 9, Batch 1300, Loss: 1.087\n",
      "Epoch 9, Batch 1400, Loss: 1.073\n",
      "Epoch 9, Batch 1500, Loss: 1.120\n",
      "Epoch 9, Batch 1600, Loss: 1.089\n",
      "Epoch 9, Batch 1700, Loss: 1.119\n",
      "Epoch 9, Batch 1800, Loss: 1.071\n",
      "Epoch 9, Batch 1900, Loss: 1.039\n",
      "Epoch 9, Batch 2000, Loss: 1.087\n",
      "Epoch 9, Batch 2100, Loss: 1.104\n",
      "Epoch 9, Batch 2200, Loss: 1.130\n",
      "Epoch 9, Batch 2300, Loss: 1.061\n",
      "Epoch 9, Batch 2400, Loss: 1.073\n",
      "Epoch 9, Batch 2500, Loss: 1.148\n",
      "Epoch 9, Batch 2600, Loss: 1.072\n",
      "Epoch 9, Batch 2700, Loss: 1.137\n",
      "Epoch 9, Batch 2800, Loss: 1.095\n",
      "Epoch 9, Batch 2900, Loss: 1.066\n",
      "Epoch 9, Batch 3000, Loss: 1.146\n",
      "Epoch 9, Batch 3100, Loss: 1.093\n",
      "Epoch 9, Batch 3200, Loss: 1.131\n",
      "Epoch 9, Batch 3300, Loss: 1.090\n",
      "Epoch 9, Batch 3400, Loss: 1.119\n",
      "Epoch 9, Batch 3500, Loss: 1.131\n",
      "Epoch 9, Batch 3600, Loss: 1.124\n",
      "Epoch 9, Batch 3700, Loss: 1.130\n",
      "Epoch 9, Batch 3800, Loss: 1.109\n",
      "Epoch 9, Batch 3900, Loss: 1.077\n",
      "Epoch 9, Batch 4000, Loss: 1.133\n",
      "Epoch 9, Batch 4100, Loss: 1.126\n",
      "Epoch 9, Batch 4200, Loss: 1.111\n",
      "Epoch 10, Batch 100, Loss: 0.998\n",
      "Epoch 10, Batch 200, Loss: 0.979\n",
      "Epoch 10, Batch 300, Loss: 1.024\n",
      "Epoch 10, Batch 400, Loss: 1.044\n",
      "Epoch 10, Batch 500, Loss: 1.024\n",
      "Epoch 10, Batch 600, Loss: 1.019\n",
      "Epoch 10, Batch 700, Loss: 1.036\n",
      "Epoch 10, Batch 800, Loss: 0.990\n",
      "Epoch 10, Batch 900, Loss: 1.020\n",
      "Epoch 10, Batch 1000, Loss: 1.017\n",
      "Epoch 10, Batch 1100, Loss: 1.068\n",
      "Epoch 10, Batch 1200, Loss: 1.029\n",
      "Epoch 10, Batch 1300, Loss: 1.051\n",
      "Epoch 10, Batch 1400, Loss: 1.060\n",
      "Epoch 10, Batch 1500, Loss: 1.086\n",
      "Epoch 10, Batch 1600, Loss: 1.060\n",
      "Epoch 10, Batch 1700, Loss: 1.044\n",
      "Epoch 10, Batch 1800, Loss: 1.047\n",
      "Epoch 10, Batch 1900, Loss: 1.051\n",
      "Epoch 10, Batch 2000, Loss: 1.067\n",
      "Epoch 10, Batch 2100, Loss: 1.085\n",
      "Epoch 10, Batch 2200, Loss: 1.113\n",
      "Epoch 10, Batch 2300, Loss: 1.106\n",
      "Epoch 10, Batch 2400, Loss: 1.072\n",
      "Epoch 10, Batch 2500, Loss: 1.070\n",
      "Epoch 10, Batch 2600, Loss: 1.093\n",
      "Epoch 10, Batch 2700, Loss: 1.092\n",
      "Epoch 10, Batch 2800, Loss: 1.039\n",
      "Epoch 10, Batch 2900, Loss: 1.105\n",
      "Epoch 10, Batch 3000, Loss: 1.090\n",
      "Epoch 10, Batch 3100, Loss: 1.123\n",
      "Epoch 10, Batch 3200, Loss: 1.097\n",
      "Epoch 10, Batch 3300, Loss: 1.090\n",
      "Epoch 10, Batch 3400, Loss: 1.080\n",
      "Epoch 10, Batch 3500, Loss: 1.070\n",
      "Epoch 10, Batch 3600, Loss: 1.098\n",
      "Epoch 10, Batch 3700, Loss: 1.056\n",
      "Epoch 10, Batch 3800, Loss: 1.137\n",
      "Epoch 10, Batch 3900, Loss: 1.115\n",
      "Epoch 10, Batch 4000, Loss: 1.087\n",
      "Epoch 10, Batch 4100, Loss: 1.105\n",
      "Epoch 10, Batch 4200, Loss: 1.102\n"
     ]
    }
   ],
   "source": [
    "from train_ai import ModelTrainer, get_model_info_dict, save_model_info_dict\n",
    "\n",
    "trainer = ModelTrainer()\n",
    "model_path, model = trainer.train_model(train_data, 10)\n",
    "\n",
    "model_info_dict = get_model_info_dict(model_path, train_data_path, model)\n",
    "save_model_info_dict(model_info_dict, \"DeepLearningAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1             [-1, 30, 5, 5]             840\n",
      "            Conv2d-2             [-1, 60, 5, 5]          16,260\n",
      "            Conv2d-3            [-1, 120, 5, 5]          64,920\n",
      "            Conv2d-4            [-1, 240, 5, 5]         259,440\n",
      "            Linear-5                  [-1, 512]       3,072,512\n",
      "            Linear-6                   [-1, 25]          12,825\n",
      "================================================================\n",
      "Total params: 3,426,797\n",
      "Trainable params: 3,426,797\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.09\n",
      "Params size (MB): 13.07\n",
      "Estimated Total Size (MB): 13.16\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, input_size=(3, 5, 5))  # 输入模型和输入tensor尺寸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 确定项目根目录（假设当前工作目录是项目的根目录）\n",
    "project_root = os.path.abspath(\"..\")\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "from ai import MinimaxAI, MCTSAI\n",
    "from ai.evaluate_ai import get_model_score_by_mcts, get_best_c_param\n",
    "from ai.deeplearning import DeepLearningAI\n",
    "\n",
    "# 与其他AI算法进行对战\n",
    "game_state = ((5,5), 2)\n",
    "\n",
    "# policy_model = DeepLearningAI('models/dl_model(06-22-21-18)(136090)(32-64-128-256).pth', complete_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mcts Iter 10: 100%|██████████| 100/100 [00:01<00:00, 59.03it/s]\n",
      "Mcts Iter 20: 100%|██████████| 100/100 [00:01<00:00, 57.61it/s]\n",
      "Mcts Iter 30: 100%|██████████| 100/100 [00:02<00:00, 45.56it/s]\n",
      "Mcts Iter 40: 100%|██████████| 100/100 [00:02<00:00, 39.49it/s]\n",
      "Mcts Iter 50: 100%|██████████| 100/100 [00:03<00:00, 30.86it/s]\n",
      "Mcts Iter 60: 100%|██████████| 100/100 [00:03<00:00, 31.29it/s]\n",
      "Mcts Iter 70: 100%|██████████| 100/100 [00:03<00:00, 28.68it/s]\n",
      "Mcts Iter 80: 100%|██████████| 100/100 [00:04<00:00, 24.54it/s]\n",
      "Mcts Iter 90: 100%|██████████| 100/100 [00:03<00:00, 25.46it/s]\n",
      "Mcts Iter 100: 100%|██████████| 100/100 [00:04<00:00, 21.93it/s]\n",
      "Mcts Iter 110: 100%|██████████| 100/100 [00:05<00:00, 19.80it/s]\n",
      "Mcts Iter 120: 100%|██████████| 100/100 [00:05<00:00, 19.78it/s]\n",
      "Mcts Iter 130: 100%|██████████| 100/100 [00:05<00:00, 19.34it/s]\n",
      "Mcts Iter 140: 100%|██████████| 100/100 [00:05<00:00, 17.19it/s]\n",
      "Mcts Iter 150: 100%|██████████| 100/100 [00:05<00:00, 17.86it/s]\n",
      "Mcts Iter 160: 100%|██████████| 100/100 [00:06<00:00, 15.18it/s]\n",
      "Mcts Iter 170: 100%|██████████| 100/100 [00:07<00:00, 13.43it/s]\n",
      "Mcts Iter 180: 100%|██████████| 100/100 [00:07<00:00, 12.88it/s]\n",
      "Mcts Iter 190: 100%|██████████| 100/100 [00:07<00:00, 12.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(180,\n",
       " {'10': 0.74,\n",
       "  '20': 0.715,\n",
       "  '30': 0.725,\n",
       "  '40': 0.67,\n",
       "  '50': 0.69,\n",
       "  '60': 0.645,\n",
       "  '70': 0.595,\n",
       "  '80': 0.575,\n",
       "  '90': 0.645,\n",
       "  '100': 0.53,\n",
       "  '110': 0.565,\n",
       "  '120': 0.535,\n",
       "  '130': 0.54,\n",
       "  '140': 0.53,\n",
       "  '150': 0.545,\n",
       "  '160': 0.61,\n",
       "  '170': 0.47,\n",
       "  '180': 0.545,\n",
       "  '190': 0.465})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DeepLearningAI(model_path, complete_mode=False)\n",
    "get_model_score_by_mcts(model, game_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mcts Iter 10: 100%|██████████| 100/100 [00:01<00:00, 67.83it/s]\n",
      "Mcts Iter 20: 100%|██████████| 100/100 [00:01<00:00, 59.77it/s]\n",
      "Mcts Iter 30: 100%|██████████| 100/100 [00:02<00:00, 44.32it/s]\n",
      "Mcts Iter 40: 100%|██████████| 100/100 [00:02<00:00, 38.57it/s]\n",
      "Mcts Iter 50: 100%|██████████| 100/100 [00:02<00:00, 36.61it/s]\n",
      "Mcts Iter 60: 100%|██████████| 100/100 [00:03<00:00, 32.69it/s]\n",
      "Mcts Iter 70: 100%|██████████| 100/100 [00:03<00:00, 27.44it/s]\n",
      "Mcts Iter 80: 100%|██████████| 100/100 [00:03<00:00, 26.53it/s]\n",
      "Mcts Iter 90: 100%|██████████| 100/100 [00:04<00:00, 22.69it/s]\n",
      "Mcts Iter 100: 100%|██████████| 100/100 [00:04<00:00, 22.76it/s]\n",
      "Mcts Iter 110: 100%|██████████| 100/100 [00:04<00:00, 20.18it/s]\n",
      "Mcts Iter 120: 100%|██████████| 100/100 [00:05<00:00, 18.41it/s]\n",
      "Mcts Iter 130: 100%|██████████| 100/100 [00:05<00:00, 17.45it/s]\n",
      "Mcts Iter 140: 100%|██████████| 100/100 [00:07<00:00, 13.65it/s]\n",
      "Mcts Iter 150: 100%|██████████| 100/100 [00:06<00:00, 15.92it/s]\n",
      "Mcts Iter 160: 100%|██████████| 100/100 [00:08<00:00, 12.30it/s]\n",
      "Mcts Iter 170: 100%|██████████| 100/100 [00:07<00:00, 13.52it/s]\n",
      "Mcts Iter 180: 100%|██████████| 100/100 [00:07<00:00, 13.62it/s]\n",
      "Mcts Iter 190: 100%|██████████| 100/100 [00:08<00:00, 12.44it/s]\n",
      "Mcts Iter 200: 100%|██████████| 100/100 [00:07<00:00, 12.71it/s]\n",
      "Mcts Iter 210: 100%|██████████| 100/100 [00:08<00:00, 12.16it/s]\n",
      "Mcts Iter 220: 100%|██████████| 100/100 [00:09<00:00, 11.02it/s]\n",
      "Mcts Iter 230: 100%|██████████| 100/100 [00:09<00:00, 10.63it/s]\n",
      "Mcts Iter 240: 100%|██████████| 100/100 [00:09<00:00, 10.01it/s]\n",
      "Mcts Iter 250: 100%|██████████| 100/100 [00:10<00:00,  9.83it/s]\n",
      "Mcts Iter 260: 100%|██████████| 100/100 [00:10<00:00,  9.20it/s]\n",
      "Mcts Iter 270: 100%|██████████| 100/100 [00:10<00:00,  9.42it/s]\n",
      "Mcts Iter 280: 100%|██████████| 100/100 [00:11<00:00,  8.59it/s]\n",
      "Mcts Iter 290: 100%|██████████| 100/100 [00:12<00:00,  8.00it/s]\n",
      "Mcts Iter 300: 100%|██████████| 100/100 [00:12<00:00,  8.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(290,\n",
       " {'10': 0.815,\n",
       "  '20': 0.76,\n",
       "  '30': 0.73,\n",
       "  '40': 0.655,\n",
       "  '50': 0.725,\n",
       "  '60': 0.685,\n",
       "  '70': 0.63,\n",
       "  '80': 0.63,\n",
       "  '90': 0.57,\n",
       "  '100': 0.51,\n",
       "  '110': 0.565,\n",
       "  '120': 0.59,\n",
       "  '130': 0.59,\n",
       "  '140': 0.52,\n",
       "  '150': 0.595,\n",
       "  '160': 0.56,\n",
       "  '170': 0.61,\n",
       "  '180': 0.54,\n",
       "  '190': 0.5,\n",
       "  '200': 0.54,\n",
       "  '210': 0.58,\n",
       "  '220': 0.6,\n",
       "  '230': 0.54,\n",
       "  '240': 0.58,\n",
       "  '250': 0.56,\n",
       "  '260': 0.535,\n",
       "  '270': 0.585,\n",
       "  '280': 0.56,\n",
       "  '290': 0.575,\n",
       "  '300': 0.52})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DeepLearningAI(r\"models\\2024-06-28\\dl_model(06-28-15-00)(136090).pth\", complete_mode=False)\n",
    "\n",
    "get_model_score_by_mcts(model, game_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "get_best_c_param(game_state, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [08:57<00:00, 48.88s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8,\n",
       " {'0.8 : 0.0': 604.5,\n",
       "  '0.8 : 0.1': 559.5,\n",
       "  '0.8 : 0.2': 504.0,\n",
       "  '0.8 : 0.3': 520.5,\n",
       "  '0.8 : 0.4': 534.0,\n",
       "  '0.8 : 0.5': 537.5,\n",
       "  '0.8 : 0.6': 530.0,\n",
       "  '0.8 : 0.7': 509.5,\n",
       "  '0.8 : 0.8': 513.5,\n",
       "  '0.8 : 0.9': 520.5,\n",
       "  '0.8 : 1.0': 532.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_best_c_param(game_state, 0.8)\n",
    "# :XTvzUQcSJ6i2HS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mcts Iter 10: 100%|██████████| 100/100 [00:03<00:00, 26.27it/s]\n",
      "Mcts Iter 20: 100%|██████████| 100/100 [00:04<00:00, 24.23it/s]\n",
      "Mcts Iter 30: 100%|██████████| 100/100 [00:04<00:00, 20.78it/s]\n",
      "Mcts Iter 40: 100%|██████████| 100/100 [00:04<00:00, 20.23it/s]\n",
      "Mcts Iter 50: 100%|██████████| 100/100 [00:05<00:00, 18.87it/s]\n",
      "Mcts Iter 60: 100%|██████████| 100/100 [00:05<00:00, 17.26it/s]\n",
      "Mcts Iter 70: 100%|██████████| 100/100 [00:06<00:00, 15.42it/s]\n",
      "Mcts Iter 80: 100%|██████████| 100/100 [00:06<00:00, 14.55it/s]\n",
      "Mcts Iter 90: 100%|██████████| 100/100 [00:06<00:00, 14.62it/s]\n",
      "Mcts Iter 100: 100%|██████████| 100/100 [00:07<00:00, 13.54it/s]\n",
      "Mcts Iter 110: 100%|██████████| 100/100 [00:08<00:00, 12.19it/s]\n",
      "Mcts Iter 120: 100%|██████████| 100/100 [00:08<00:00, 11.72it/s]\n",
      "Mcts Iter 130: 100%|██████████| 100/100 [00:08<00:00, 11.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, {'10': 0.74, '20': 0.725, '30': 0.705, '40': 0.655, '50': 0.65, '60': 0.56, '70': 0.515, '80': 0.59, '90': 0.505, '100': 0.525, '110': 0.545, '120': 0.49, '130': 0.415})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(get_model_score_by_mcts(MCTSAI(100, 0.8, complete_mode=False), game_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 8/999 [26:49<55:23:30, 201.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, {10: 0.735, 20: 0.69, 30: 0.67, 40: 0.565, 50: 0.545, 60: 0.56, 70: 0.555, 80: 0.5, 90: 0.49})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "policy_model = DeepLearningAI(r\"models\\2024-06-28\\dl_model(06-28-15-00)(136090).pth\", complete_mode=False)\n",
    "mcts_model = MCTSAI(100, c_param=0.5, policy_net=policy_model, complete_mode=False)\n",
    "\n",
    "get_model_score_by_mcts(mcts_model, game_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mcts Iter 10: 100%|██████████| 100/100 [00:00<00:00, 101.95it/s]\n",
      "Mcts Iter 20: 100%|██████████| 100/100 [00:01<00:00, 56.16it/s]\n",
      "Mcts Iter 30: 100%|██████████| 100/100 [00:02<00:00, 41.09it/s]\n",
      "Mcts Iter 40: 100%|██████████| 100/100 [00:02<00:00, 42.13it/s]\n",
      "Mcts Iter 50: 100%|██████████| 100/100 [00:04<00:00, 22.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, {'10': 0.68, '20': 0.66, '30': 0.59, '40': 0.505, '50': 0.485})\n"
     ]
    }
   ],
   "source": [
    "minimax_ai = MinimaxAI(6, game_state, complete_mode=True)\n",
    "\n",
    "print(get_model_score_by_mcts(minimax_ai, game_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_model_score_by_mcts in module ai.test_ai:\n",
      "\n",
      "get_model_score_by_mcts(test_model: ai.ai_algorithm.AIAlgorithm, game_state)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(get_model_score_by_mcts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 深度学习AI\n",
    "deep_learning_dict = {\n",
    "                      # {10: 0.7, 20: 0.68, 30: 0.47}\n",
    "                      # \n",
    "                      \"models/chess_ai_model(06-13-19).pth\": 20, # 未知\n",
    "                      # {10: 0.7, 20: 0.51, 30: 0.43}\n",
    "                      # \n",
    "                      \"models/chess_ai_model(06-15-17)(1506).pth\": 20, # MCTSAI(1000, flag=True) 训练100轮\n",
    "                      # {10: 0.75, 20: 0.65, 30: 0.47}\n",
    "                      # \n",
    "                      'models/chess_ai_model(06-15-17)(1471).pth': 20, # MCTSAI(10000, flag=True) 训练100轮\n",
    "                      # {10: 0.63, 20: 0.41}\n",
    "                      'models/chess_ai_model(06-15-17)(936).pth': 10, # MCTSAI(10000, flag=True) 训练100轮并去重\n",
    "                      # {10: 0.82, 20: 0.76, 30: 0.65, 40: 0.57, 50: 0.56, 60: 0.44}\n",
    "                      # \n",
    "                      'models/chess_ai_model(06-15-17)(15475).pth': 50, # MCTSAI(1000, flag=True) 训练1000轮\n",
    "                      # {10: 0.84, 20: 0.58, 30: 0.57, 40: 0.61, 50: 0.56, 60: 0.41}\n",
    "                      'models/chess_ai_model(06-15-17)(9598).pth': 50, # MCTSAI(1000, flag=True) 训练1000轮并去重\n",
    "                      # {10: 0.32}\n",
    "                      \"models/chess_ai_model(06-16-15)(1400).pth\": 0, # MCTSAI(10000, flag=False) 训练100轮(没标错, 真是0分)\n",
    "                      # {10: 0.55, 20: 0.48}\n",
    "                      'models/chess_ai_model(06-16-20)(1542).pth': 10, # MCTSAI(1000, flag=False) 训练100轮\n",
    "\n",
    "                      # {10: 0.82, 20: 0.73, 30: 0.67, 40: 0.52, 50: 0.46}\n",
    "                      # \n",
    "                      'models/chess_ai_model(06-17-12)(15475).pth': 40, # MCTSAI(1000, flag=True) 训练1000轮, 使用三层32-64-128卷积层, 128-256-25全连接层\n",
    "                      # {10: 0.72, 20: 0.39}\n",
    "                      'models/chess_ai_model(06-17-12)(1506).pth': 10, # MCTSAI(1000, flag=True) 训练100轮, 使用三层32-64-128卷积层, 128-256-25全连接层\n",
    "                      \n",
    "                      # {10: 0.9, 20: 0.84, 30: 0.72, 40: 0.6, 50: 0.48}\n",
    "                      # {10: 0.88, 20: 0.83, 30: 0.69, 40: 0.68, 50: 0.57, 60: 0.65, 70: 0.61, 80: 0.63, 90: 0.49}\n",
    "                      # \n",
    "                      'models/chess_ai_model(06-17-13)(15475).pth': 80, # MCTSAI(1000, flag=True) 训练1000轮, 使用三层16-32-64卷积层, 64-128-25全连接层\n",
    "                      }\n",
    "\n",
    "                    \n",
    "# deep_learning_ai_0 = DeepLearningAI(\"models/chess_ai_model(06-15-17)(936).pth\") \n",
    "\n",
    "# 测试AI\n",
    "test_minimax = MinimaxAI(6) # score: 50\n",
    "test_mcts_0 = MCTSAI(1000) # \n",
    "test_mcts_1 = MCTSAI(100, flag=True) # score: 50\n",
    "test_mcts_2 = MCTSAI(100, flag=False) # score: 80\n",
    "test_mcts_3 = MCTSAI(60, flag=True) # 50, {10: 0.82, 20: 0.76, 30: 0.53, 40: 0.57, 50: 0.51, 60: 0.32}\n",
    "test_mcts_4 = MCTSAI(60, flag=False) # 110, {10: 0.86, 20: 0.86, 30: 0.77, 40: 0.63, 50: 0.58, 60: 0.63, 70: 0.58, 80: 0.52, 90: 0.51, 100: 0.5, 110: 0.52, 120: 0.45}\n",
    "test_mcts_5 = MCTSAI(10, flag=True) # 0, {10: 0.45}\n",
    "test_mcts_6 = MCTSAI(10, flag=False) # 50, {10: 0.76, 20: 0.69, 30: 0.57, 40: 0.56, 50: 0.5, 60: 0.4}\n",
    "test_mcts_7 = MCTSAI(500, flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toolenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
